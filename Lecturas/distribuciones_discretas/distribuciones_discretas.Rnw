\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{bigints}
\usepackage[latin1]{inputenc}
\usepackage{geometry}
\usepackage{amsmath} 
\usepackage{mathtools}
%\usepackage[spanish]{babel}
\usepackage{latexsym}
\geometry{verbose,tmargin=1.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage{graphicx}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bb}[1]{\textbf{#1}}
\DeclareMathOperator{\rank}{\textbf{rango}}
\DeclareMathOperator{\proy}{\textbf{proy}}
\DeclareMathOperator{\nulll}{\textbf{nul}}
\DeclareMathOperator{\diag}{\textbf{diag}}
\DeclareMathOperator{\col}{\textbf{col}}
\DeclareMathOperator{\fila}{\textbf{fila}}
\DeclareMathOperator{\dimm}{dim}
\DeclareMathOperator{\Traz}{Tr}
%\theoremstyle{definition}
\everymath{\displaystyle}
\newtheorem{ejemplo}{{Ejemplo }}[section]
\newtheorem{defi}{{Definici\'on}}[section]
\newtheorem{pros}{{Proposici\'on}}[section]
\newtheorem{cor}{{Corolario}}[section]
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(ggplot2)
library(grid)
#library(animation)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@

%\title{\underline{\textbf{Notas de mat\'ematica}}}
%\date{}
%\maketitle
\hspace*{0.5\linewidth}
\begin{minipage}{0.6\linewidth}
Curso: C\'alculo de probabilidades CM-1H2\par
Distribuciones importantes \par
\end{minipage}


\vspace {0.5cm}


\section{Distribuciones discretas }
 
 
 Algunas de la m\'as importantes variables aleatorias discretas, se listan a continuaci\'on:


\subsection{Distribuci\'on de masa puntual} $X$ tiene una distribuci\'on de masa puntual en $a$, $X \sim \delta_a$, si $\mathbb{P}(X =a) =1$, en el caso que 

\[
F_X(x) = \begin{cases}
0 & x < a \\
1 & x \geq a.
\end{cases}
\]

La funci\'on de masa de probabilidad es $p_X(x) =1$ para $x = a$ y $0$ en otros casos.


\subsection{Distribuci\'on uniforme discreta} Sea $k > 1$ un n\'umero entero. Supongamos que $X$ tiene \texttt{PMF} dado por

\[
p_X(x) = \begin{cases}
1/k & \mbox{para} \ 1, \dots ,k\\
0 & \mbox{ en otros casos.}
\end{cases}
\]

\vspace{0.2cm}

Decimos que $X$ tiene una distribuci\'on uniforme sobre $\{1, \dots, k \}$.

\vspace{0.3cm}

\subsection{Distribuci\'on de Bernoulli} Sea $X$ que representa una lanzamiento de una moneda. Entonces $\mathbb{P}(X = 1) = p$ y $\mathbb{P}(X= 0) = 1 - p$ para alg\'un $p \in [0,1]$, entonces decimos que $X$ tiene una distribuci\'on de Bernoulli, escrita como $X \sim\mbox{Bernoulli}(p)$. La funci\'on \mbox{probabilidad} es $p_X(x) = p^{x}(1 - p)^{1-x}$ para $x \in \{0,1\}$.

\vspace{0.3cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.45]{p1.png}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[scale=.45]{p2.png}
\end{figure}


\vspace{0.2cm}

La distribuci\'on de \texttt{Bernoulli(p)} tiene una media:

\[
\mathbb{E}(X) = \sum_{x = 0,1}xp(X =x) = 0(1 -p) + 1p = p.
\]

\vspace{0.2cm}

Adem\'as:

\vspace{0.2cm}

\[
\mathbb{E}(X)^2 = \sum_{x = 0,1}x^2p(X =x) = 0^2(1 -p) + 1^2p = p
\]

y as\'i la varianza:

\[
\text{Var}(X) = \mathbb{E}(X^2) - [\mathbb{E}(X)]^2 = p - p^2 = p(1 -p).
\]

\vspace{0.2cm}

La varianza  es maximizada para el valor de $p = 1/2$ y cada momento es el mismo $\mathbb{E}(X^{\alpha}) = 0^{\alpha}(1 -p) + 1^{\alpha}p = p$.
\vspace{0.3cm}

\subsection{Distribuci\'on binomial} Supongamos que tenemos una moneda, que cae cara con una probabilidad $p$ para alg\'un $0 \leq p \leq 1$. Lanzamos la moneda $n$ veces y sea $X$ el n\'umero de caras. Asumimos que estos lanzamientos son independientes. El PMF  de X, $p_X(x) = \mathbb{P}(X = x)$  para esta distribuci\'on es:

\vspace{0.2cm}


\[
p_X(x) = \begin{cases}
\binom{n}{x}p^{x}(1 - p)^{x} & \mbox{para} \ x =0, \dots ,n\\
0 & \mbox{ en otros casos.}
\end{cases}
\]

Esta es la suma de $n$ variables aleatorias independientes de Bernoulli. Se denota como $X \sim\mbox{Binomial}(n, p)$.


\vspace{0.2cm}

Desde que la esperanza es un operador lineal, la esperanza de una distribuci\'on Binomial, es la suma de las esperanzas de las distribuciones de Bernoulli involucradas, as\'i si $X$ es una distribuci\'on \texttt{Binomial(n,p)}:

\[
\mathbb{E}(X) = np,
\]

y desde que la varianza de la suma de variables independientes es la suma de varianza,

\[
\text{Var}(X) = np(1 -p).
\]


\begin{figure}[ht]
\centering
\includegraphics[scale=.45]{p3.png}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[scale=.45]{p4.png}
\end{figure}


\vspace{0.3cm}


\subsection{Distribuci\'on binomial negativa} Replicar un experimento  de \texttt{Bernoulli(p)} de forma independiente hasta que ocurre el r-\'esimo  \'exito $(r \geq 1)$ Sea $X$  el n\'umero de pruebas  en el cual se  produce el r-\'esimo \'exito. Entonces se dice que $X$ tiene una distribuci\'on  \texttt{binomial negativa(r,p)}.

\vspace{0.2cm}

Existe otra definici\'on de distribuci\'on binomial negativa, a saber, la distribuci\'on del n\'umero de fallos antes del r-\'esimo \'exito. (Esta es la definici\'on empleada por R, en los c\'alculos).

La relaci\'on entre las dos definiciones es bastante simple. Si $X$ es binomial negativo en el sentido usual  y $F$ es binomial negativo en el sentido de R, entonces:

\[
F = X -r
\]

?` Cu\'al es la probabilidad de que el r-\'esimo  \'exito ocurre en la prueba  $t$, para $t \geq r$ ?.  Para que esto suceda, debe haber $t- r$ fracasos y $r -1$ \'exitos, en las primeras $t - 1$ pruebas, con un \'exito en la prueba $t$.

Por independencia, esto sucede con la probabilidad binomial para $r- 1$ \'exitos en las primeras $t -1$ pruebas, por  la probabilidad $p$ de \'exito en la pruena $t$:

\[
\mathbb{P}(X =t) = \binom{t -1}{r -1}p^r(1 -p)^{t -r}\ \ (t \geq r).
\]

de esta manera la probabilidad es cero, cuando $t < r$. El caso especial $r = 1$ es llamada \texttt{distribuci\'on geom\'etrica}. Es decir, $X$ tiene una distribuci\'on geom\'etrica con par\'ametro $p \in (0,1)$, denotada  como $X \sim \mbox{Geometrica}(p)$, si

\[
\mathbb{P}(X = k) = p(1 -p)^{k -1}, \ \ k \geq 1.
\]

\vspace{0.2cm}

Tenemos a partir de este resultado que 

\vspace{0.2cm}

\[
\sum_{k = 1}^{\infty}\mathbb{P}(X= k) = p\sum_{k = 1}^{\infty}(1 - p)^{k} = \frac{p}{ 1 - (1 - p)} = 1.
\]

\vspace{0.2cm}

$X$ es el n\'umero de lanzamientos necesarios hasta que  la primera cara salga, cuando una moneda es lanzada. La media de una distribuci\'on geom\'etrica, tiene el valor de:

\[
\mathbb{E}(X) = \sum_{t =1}^{\infty}tp(1 - p)^{t -1} = \frac{1}{p}.
\]

Adem\'as que se tiene:

\[
\text{Var}(X) = \mathbb{E}(X^2) - [\mathbb{E}(X)]^2 = \frac{1 - p}{p}.
\]
 
 
\vspace{0.2cm}

La distribuci\'on \texttt{binomial negativa(r,p)} es la suma de $r$ de variables aleatorias  geom\'etricas independientes \texttt{Geometrica(p)}. Como la esperanza es un operador lineal positivo, se concluye que la media de \texttt{binomial negativa(r,p)} es $r$ veces la media que \texttt{Geometrica(p)} y la varianza de una suma independiente es la suma de varianza, as\'i:

\vspace{0.2cm}

Si $X \sim \texttt{binomial negativa(r,p)}$, entonces:

\[
\mathbb{E}(X) = \frac{r}{p}\ \ \text{y}\ \ \text{Var}(X) = \frac{r(1 -p)}{p^2}.
\]
 
 
\vspace{0.3cm}


\subsection{Distribuci\'on multinomial} En un sentido, la distribuci\'on multinominal generaliza la distribuci\'on binomial a experimentos aleatorios independientes con m\'as de dos resultados. Es la distribuci\'on de un vector que cuenta cu\'antas veces se produce cada resultado. Se denota como $X \sim\mbox{Multinomial}(n, \textbf{p})$.

\vspace{0.2cm}

Un vector aleatorio multinomial es un $m$ -vector \textbf{X}  de resultados en una secuencia de $n$ repeticiones independientes de un experimento aleatorio con $m$ resultados distintos.

\vspace{0.2cm}

Si el experimento tiene $m$ resultados posibles y el i-\'esimo resultado tiene probabilidad $p_i$, entonces la funci\'on de masa de probabilidad \texttt{Multinomial(n, \textbf{p})} viene dada por:

\[
\mathbb{P}(\textbf{X} = (k_1, \dots, k_m)) = \frac{n!}{k_1!k_2!\cdots k_m!}p_1^{k_1}\cdot p_2^{k_2}\cdots p_m^{k_m}.
\]

donde $k_1 + k_2 + \cdots + k_m = n$.

\vspace{0.2cm}

Si contamos el resultado $k$ como \texttt{\'exito}, entonces es obvio que cada $X_k$ es simplemente una variable aleatoria  binomial (n, pk). Pero los componentes no son independientes, ya que suman a n.


\vspace{0.3cm}

\subsection{Distribuci\'on de Rademacher} La distribuci\'on \texttt{Rademacher(p)} es una variaci\'on de la distribuci\'on de Bernoulli, $1$ todav\'ia indica \'exito, pero el fracaso se codifica como $-1$. Si $Y$ es una variable aleatoria \texttt{Bernoulli(p)}, entonces $X = 2Y -1$ es una variable aleatoria \texttt{Rademacher(p)}. La funci\'on de masa de probabilidad es:

\[
\mathbb{P}(X =x) = \begin{cases}
p & x =1 \\
1 -p & x = -1
\end{cases}
\]

\vspace{0.2cm}

Una variable aleatoria $X$ \texttt{Rademacher(p)} tiene una media:

\[
\mathbb{E}(X) = \sum_{x = -1,1}xp(X =x) = -1(1 -p) + 1p = 2p -1.
\]

Adem\'as  $X^2 = 1$, as\'i:

\[
\mathbb{E}(X^2) = 1,
\]

por tanto la varianza es:

\[
\text{Var}(X) = \mathbb{E}(X^2) -[\mathbb{E}(X)]^2 = 1 - (2p -1)^2 = 4p(1 - p).
\]

Una secuencia de sucesivas sumas de variables aleatoria independientes de \texttt{Rademacher(p)}  es llamado \textbf{camino aleatorio}. Esto es, si $X_i$ son id\'enticamente distribuidas a  variables aleatorias \texttt{Rademacher(1/2)}, la secuencia $S_1, S_2, \dots$ es un camino aleatorio, donde:

\[
S_n = X_1 + \cdots X_n.
\]

Desde que el operador esperanza es un operador lineal:

\[
\mathbb{E}(S_n)  = 0,\ \ \text{para todo}\ \  n,
\]

\vspace{0.2cm}

y desde que la varianza  de la suma de variables aleatorias independientes es la suma de varianzas:

\[
\text{Var}(S_n) = n, \ \ \text{para todo}\ \ n.
\]
\vspace{0.3cm}

\subsection{Distribuci\'on de Poisson} Una variable aleatoria de Poisson $N$ modela el recuento de  \'exitos cuando la probabilidad de \'exitos es peque\~na y el n\'umero de pruebas independientes  es grande, de modo que la tasa de \'exito promedio $\mu$. Se denota esta variable aleatoria  como $X \sim \mbox{Poisson} (\mu)$ si:

\[
\mathbb{P}(N = k) = e^{-\mu}\frac{\mu^{k}}{k!} \ \ k = 0,1, \dots.
\]


As\'i :

\[
\mathbb{E}(N) = \mu \ \ \text{y}\ \ \text{Var}(N) = \mu.
\]


\vspace{0.2cm}


La distribuci\'on de Poisson es usado a menudo como un modelo para eventos, como decaimiento radiactivo y accidentes de tr\'afico.


\vspace{0.2cm}

Ladislaus von Bortkiewicz  se refiri\'o a la distribuci\'on de Poisson como \texttt{La Ley de los n\'umeros peque\~nos}. Se puede pensar esto, como un l\'imite peculiar de las distribuciones binomiales. Consideremos una secuencia de variables \texttt{binomiales(n, p)}, donde la probabilidad $p$ de \'exito va a cero, pero el n\'umero $n$ de pruebascrece de tal manera que $np = \mu $ permanece fijo. Entonces tenemos la siguiente proposici\'on:

\vspace{0.2cm}


\begin{pros}

\normalfont Para cada $k$:

\[
\lim_{n \rightarrow \infty}\text{Binomial}(n, \mu/k)(k) = \text{Poisson}(\mu)(k).
\]
\end{pros}

\end{document}
