\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{bigints}
\usepackage[latin1]{inputenc}
\usepackage{geometry}
\usepackage{amssymb}
\usepackage{amsmath} 
\usepackage{mathtools}
%\usepackage[spanish]{babel}
\usepackage{latexsym}
\geometry{verbose,tmargin=1.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage{graphicx}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bb}[1]{\textbf{#1}}
\DeclareMathOperator{\rank}{\textbf{rango}}
\DeclareMathOperator{\proy}{\textbf{proy}}
\DeclareMathOperator{\nulll}{\textbf{nul}}
\DeclareMathOperator{\diag}{\textbf{diag}}
\DeclareMathOperator{\col}{\textbf{col}}
\DeclareMathOperator{\fila}{\textbf{fila}}
\DeclareMathOperator{\dimm}{dim}
\DeclareMathOperator{\Traz}{Tr}
%\theoremstyle{definition}
\everymath{\displaystyle}
\newtheorem{ejemplo}{{Ejemplo }}[section]
\newtheorem{teo}{{Teorema}}[section]
\newtheorem{defi}{{Definici\'on}}[section]
\newtheorem{pros}{{Proposici\'on}}[section]
\newtheorem{cor}{{Corolario}}[section]
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(ggplot2)
library(grid)
#library(animation)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@

%\title{\underline{\textbf{Notas de mat\'ematica}}}
%\date{}
%\maketitle
\hspace*{0.5\linewidth}
\begin{minipage}{0.6\linewidth}
Curso: C\'alculo de Probabilidades CM -1H2\par
Desigualdades y Teoremas L\'imites \par
\end{minipage}


\vspace {0.5cm}


\section{Desigualdades y teoremas l\'imites}

Las desigualdades y el comportamiento asint\'otico de secuencias de variables aleatorias es un parte importante de la teoria de las probabilidades. El principal contexto  involucra una secuencia $X_1, X_2, \dots $ de variables aleatorias independientes id\'enticamente distribuidas con media $\mu$ y varianza $\sigma^2$.  

\vspace{0.2cm}

Existen importantes desigualdades que se pueden aplicarse en este escenario: la desigualdad de Markov, utiliza s\'olo la esperanza de una variable aleatoria. La desigualdad de Chebychev, utiliza tanto la esperanza como la varianza de una variable aleatoria y normalmente genera cotas  m\'as estrictas que el resultado de Markov.

\subsection{Desigualdad de Markov} 

\begin{teo}
\normalfont Sea $X$ es una variable no negativa y supongamos que  $\mathbb{E}(X)$ existe. Entonces se cumple:

\[
\mathbb{P}(X > a)\leq \frac{\mathbb{E}(X)}{a}.
\]

Para alg\'un $a > 0$.
\end{teo}


\vspace{0.3cm}

Este resultado es ilustrado mediante el siguiente gr\'afico:


\begin{figure}[h]
\centering
\includegraphics[scale=.55]{m1.png}
\end{figure}


\vspace{0.2cm}


Donde  la parte a) muestra el \texttt{funci\'on densidad de probabilidad} de una variable aleatoria no negativa $X$. La parte b) muestra la \texttt{funci\'on de masa de probabilidad } de una variable $Y_a$, que se construye de la siguiente manera: Toda la masa de probabilidad en el PDF de $X$ que se encuentra entre $0$ y $a$ se le asigna  $0$  y toda la masa que se encuentra por encima de $a$ se le asigna a $a $, puesto que la masa se desplaza a la izquierda, la esperanza s\'olo puede disminuir.

\vspace{0.3cm}

Vayamos con la prueba.

\vspace{0.3cm}


Fijemos un n\'umero positivo $a$ y consideremos una variable aleatoria $Y_a$ definida como:

\[
Y_a = \begin{cases}
0, & \text{si}\ \ X  < a\\
a, & \text{si}\ \ X \geq a
\end{cases}
\]

Luego: 

\[
Y_a \leq X
\]

Lo que implica, si tomamos esperanzas en ambos lados: 

\vspace{0.2cm}

\[
\mathbb{E}(Y_a) \leq \mathbb{E}(X)
\]

\vspace{0.2cm}

\[
\mathbb{E}(X) \geq \mathbb{E}(Y_a) = a\mathbb{P}(Y_a = a) = a\mathbb{P}(X \geq a)
\]

\vspace{0.2cm}

Por tanto : 

\vspace {0.2cm}

\[
a\mathbb{P}(X \leq a) \leq \mathbb{E}(X).
\]

\begin{ejemplo}
\normalfont Sea $X$ la variable aleatoria que denota la edad  (en a\~nos) de un ni\~no de un determinado colegio. Si la edad promedio de edad de ese colegio es $12.5$ a\~nos, entonces usando la desigualdad de Markov, la probabilidad de  que un ni\~no es tiene por lo menos 20 a\~nos es: 

\vspace{0.2cm}

\[
\mathbb{P}(X \geq 20) \leq 12.5/20 = 0.6250.
\]

\vspace{0.5cm}

En general si $X$ es una variable aleatoria y $h$ una funci\'on no decreciente  y no negativa. La esperanza $h(X)$ asumiendo que existe, es dada por:


\[
\mathbb{E}(h(X)) = \bigintsss_{-\infty}^{\infty}h(u)f_{X}(u)du,
\]

\vspace{0.2cm}

y escribimos:


\[
\bigintsss_{-\infty}^{\infty}h(u)f_{X}(u)du \geq \bigintsss_{a}^{\infty}h(u)f_{X}(u)du \geq h(a)\bigintsss_{t}^{\infty}f_{X}(u)du = h(a)\mathbb{P}(X \geq a).
\]

\vspace{0.2cm}

Esto conduce a la desigualdad de Markov: $\mathbb{P}(X \geq a) \leq \dfrac{\mathbb{E}(h(X))}{h(a)} $ para $a > 0$.
\end{ejemplo}

\subsection{Desigualdad de Chebyshev}

\vspace{0.2cm}

La desigualdad de Chebyshev, dice que si una variable aleatoria tiene una peque\~na varianza, entonces la probabilidad que toma un valor grande lejos  de la media es tambi\'en peque\~no.

\begin{teo}
\normalfont  Sea $\mu = \mathbb{E}(X)$ y $\sigma^2 = \text{Var}(X)$. Entonces para un $t > 0$

\[
\mathbb{P}(\vert X- \mu \vert \geq t) \leq \frac{\sigma^2}{t^2}\qquad y \qquad \mathbb{P}(\vert Z \vert \geq k) \leq \frac{1}{k^2}.
\]

\vspace{0.3cm}

donde $Z = (X - \mu)/\sigma$. En particular $\mathbb{P}(\vert Z \vert > 2) \leq 1/4$ \qquad y \qquad  $\mathbb{P}(\vert Z \vert > 3) \leq 1/9$.


\vspace{0.3cm}

Aplicando la desigualdad de Markov, podemos concluir que:

\vspace{0.3cm}

\[
\mathbb{P}((X -\mu)^2 \leq t^2) \leq \dfrac{\mathbb{E}(X -\mu)^2}{t^2} = \dfrac{\sigma^2}{t^2}.
\]


Para completar el evento $(X - \mu)^2 \geq t^2$ es id\'entico al evento $\vert X - \mu\vert \geq t$, as\'i:

\vspace{0.2cm}

\[
\mathbb{P}(\vert X - \mu \vert \leq t) = \mathbb{P}((X -\mu)^2 \leq t^2) \leq \dfrac{\sigma^2}{t^2}.
\]

\vspace{0.2cm}

Para la segunda parte se coloca $t = k\sigma$.
\end{teo}

\vspace{0.3cm}

\begin{ejemplo}
\normalfont Volvamos al  ejemplo (1.1) de los ni\~no de la escuela  y recalculamos el l\'imite usando la desigualdad de Chebychev. En este caso tambi\'en necesitamos la varianza de las edades de los ni\~nos, que tomamos como $3$. Buscamos la probabilidad de que un ni\~no en la escuela pueda ser mayor  que $20$. Primero debemos poner esto en la forma requerida por La ecuaci\'on de Chebychev:

\[
\mathbb{P}(X \geq 20) = \mathbb{P}[(X -\mathbb{E}(X)) \geq (20 - \mathbb{E}(X))] =  \mathbb{P}[(X -\mathbb{E}(X)) \geq 7.5)].
\]

\vspace{0.2cm}

Sin embargo:

\vspace{0.2cm}

\[
 \mathbb{P}[(X - 12.5) \geq 7.5] \neq   \mathbb{P}[\vert X -12.5\vert \geq 7.5)] = \mathbb{P}(5 \leq X \geq 20).
\]


\vspace{0.2cm}

As\'i no podemos aplicar la desigualdad de Chebyshev para calcular directamente $\mathbb{P}(X \geq 20)$. En efecto, calculamos, una cota para $\mathbb{P}(5 \leq X \geq 20)$ y obtenemos:

\[
 \mathbb{P}[\vert X - \mathbb{E}(X) \vert \geq 7.5)] \leq \frac{3}{(7.5)^2} = 0.0533, 
\]

\vspace{0.2cm}

que sigue siendo un l\'imite mucho m\'as estricto que el obtenido anteriormente.
\end{ejemplo}

\vspace{0.3cm}

\subsection{Desigualdad de Hoeffding}

Empecemos  con un resultado importante:

\vspace{0.2cm}

\begin{pros}
\normalfont Supongamos que $\mathbb{E}(X) = 0$ y que $a \leq x \leq b$. Entonces:

\[
\mathbb{E}(e^{tX}) \leq e^{t^2(b -a)^2/8}.
\]
\end{pros}

\vspace{0.2cm}

Para desarrollar la desigualdad de  Hoeffding, necesitamos el \texttt{m\'etodo de Chernoff.}

\vspace{0.2cm}

\begin{pros}
\normalfont \textbf{(M\'etodo de Chernoff)} Sea $X$ una variable aleatoria. Entonces:

\[
\mathbb{P}(X > \epsilon) \leq \inf_{t \geq 0}e^{-t\epsilon}\mathbb{E}(e^{tX}).
\]

\end{pros}

\vspace{0.2cm}

Este m\'etodo puede ser derivado de la desigualdad de Markov con  la funci\'on $h(x) = e^{tx}$ que  es la funci\'on generadora de momentos  $\mathbb{E}(e^{tX})$.

\vspace{0.2cm}


Para alg\'un $t > 0$

\vspace{0.2cm}

\[
\mathbb{P}(X >\epsilon) = \mathbb{P}(e^ X > e^{\epsilon}) = \mathbb{P}(e^{tX} > e^{t\epsilon}) \leq e^{-t\epsilon}\mathbb{E}(e^{tX}).
\]

\vspace{0.2cm}

Desde que esto es cierto para $t \geq 0$ el resultado sigue.


\vspace{0.3cm}

\begin{teo}
\normalfont (\textbf{Desigualdad de Hoeffding}) Sean $Y_1, Y_2, \dots, Y_n$ observaciones indepedientes, tal que $\mathbb{E}(Y_i)=0 $ y $a_i \leq Y_i \leq b_i$. Sea $\epsilon > 0$. Entonces, para alg\'un $ t > 0$

\vspace{0.2cm}

\[
\mathbb{P}(\vert \overline{Y}_n - \mu \vert \geq \epsilon) \leq 2e^{-2n\epsilon^2/(b -a)^2}.
\]

\end{teo}

\vspace{0.2cm}


Sin p\'erdida de generalidad, supongamos $\mu = 0$. Entonces se tiene:

\vspace{0.2cm}

\begin{align*}
\mathbb{P}(\vert \overline{Y}_n\vert \geq \epsilon) = \mathbb{P}( \overline{Y}_n \geq \epsilon) + \mathbb{P}( \overline{Y}_n\vert \leq -\epsilon) \\
= \mathbb{P}( \overline{Y}_n \geq \epsilon) + \mathbb{P}( -\overline{Y}_n \geq \epsilon)
\end{align*}

\vspace{0.2cm}

Usamos el m\'etodo de chernoff. Para alg\'un $t > 0$, desde la desigualdad de Markov, que:

\vspace{0.2cm}

\begin{align*}
\mathbb{P}( \overline{Y}_n \geq \epsilon) = \mathbb{P}\Bigl(\displaystyle\sum_{i =1}^{n}Y_i \geq n\epsilon\Bigr) = \mathbb{P}(e^{\sum_{i =1}^{n}y_i}\geq e^{n\epsilon})\\
= \mathbb{P} \Bigl(e^{t\sum_{i =1}^{n}y_i}\geq e^{tn\epsilon}\Bigr) \leq e^{-tn\epsilon}\mathbb{E}\Bigl(e^{t\sum_{i =1}^{n}y_i} \Bigr)\\
=e^{-tn\epsilon}\displaystyle\prod_{i}(e^{tY_i}) = e^{-tn\epsilon}(\mathbb{E}(e^{tY_i}))^{n}
\end{align*}


\vspace{0.2cm}

Por la proposici\'on (1.1): $\mathbb{E}(e^{tY_i}) \leq e^{t^2(b -a)^2/8}$. As\'i:


\[
\mathbb{P}( \overline{Y}_n \geq \epsilon) \leq e^{-tn\epsilon}e^{t^2n(b -a)^2/8}
\]

\vspace{0.2cm}

Esto se reduce al m\'inimo cuando $t = 4\epsilon/(b -a)^2$ dando el resultado: 

\vspace{0.2cm}

\[
\mathbb{P}( \overline{Y}_n \geq \epsilon) \leq e^{-2n\epsilon^2/(b -a)^2}
\]

\vspace{0.2cm}

Aplicando el mismo argumento a $\mathbb{P}( - \overline{Y}_n \geq \epsilon)$ se produce el resultado.

\vspace{0.2cm}

\begin{ejemplo}
\normalfont Sea $X_1, X_2, \dots, X_n \sim \mbox{Bernoulli}(p)$. Desde la desigualdad Hoeffding se tiene,


\[
\mathbb{P}(\vert \overline{X}_n -p \vert > \epsilon) \leq 2\epsilon^{-2n\epsilon^2}.
\]

\vspace{0.2cm}

donde $\overline{X}_n = n^{-1}\displaystyle\sum_{i = 1}^{n}X_i$

\vspace{0.2cm}

Sea $X_1, X_2, \dots, X_n \sim \mbox{Bernoulli}(p)$. Sea $n = 100$ y $\epsilon= .2$. Por la desigualdad de Chebyshev, tenemos

\vspace{0.2cm}

\[
\mathbb{P}(\vert \overline{X}_n - p\vert > \epsilon) \leq .0625
\]

\vspace{0.2cm}

De acuerdo a la desigualdad de Hoeffding:

\vspace{0.2cm}

\[
\mathbb{P}(\vert \overline{X}_n - p\vert > .2) \leq 2e^{-2(100)(.2)^2} = .00067
\]

\vspace{0.2cm}

que es mucho m\'as peque\~no que el resultado obtenido por la desigualdad de Chebyshev $.0625$.
\end{ejemplo}

\vspace{0.3cm}

\vspace{0.5cm}

El siguiente resultado extiende la desigualdad de Hoeffding a funciones m\'as generales $g(x_1, \dots, x_n)$. Consideramos la desigualdad de McDiarmind.

\subsection{Desigualdad de McDiarmind}

\begin{teo}
\normalfont \textbf{Teorema de McDiarmid} Sea $X_1, X_2, \dots X_n$ variables aleatorias independientes. Suponganse que

\[
\sup_{x_1,\dots, x_n, x^{'}_{i}}\Bigl\vert g(x_1,\dots, x_{i - 1}, x_i, x_{i + 1}, \dots, x_n ) - g(x_1,\dots, x_{i - 1}, x^{'}_i, x_{i + 1}, \dots, x_n )\Bigr\vert \leq c_i
\]

\vspace{0.2cm}

para $i = 1,\dots, n$. Entonces

\vspace{0.2cm}

\[
\mathbb{P}\Bigl(g(X_1,\dots, X_n) -\mathbb{E}(g(X_1,\dots, X_n))\geq \epsilon \Bigr) \leq \exp\Bigl\{-\frac{2\epsilon^2 }{\sum_{i = 1}^{n}c^{2}_i}    \Bigr\}.
\]

\end{teo}
\vspace{0.3cm}

Si tomamos $g(x_1,\dots, x_n) = n^{-1}\displaystyle\sum_{i = 1}^{n}x_i$, es la desigualdad de Hoeffding.


\begin{ejemplo}
\normalfont Supongamos que lanzamos $m$ pelotas en $n$ recipientes. ?` Qu\'e fracci\'on de los recipientes est\'an vacios?.

\vspace{0.2cm}

Sea $Z$ el n\'umero de recipientes vacios y sea $F = Z/n$ la fracci\'on de recipientes vacios. Podemos escribir $Z = \sum_{i =1}^{n}Z_i$, donde $Z_i = 1$ de un recipiente $i$ est\'a vac\'io y $Z_i = 0$ en otro caso. Entonces:


\[
\mu = \mathbb{E}(Z) = \displaystyle\sum_{i =1}^{n}\mathbb{E}(Z_i) = n(1 -1/n)^m = ne^{m\log(1 -1/n)} \approx ne^{-m/n}.
\]

\vspace{0.2cm}

y  $\theta =\mathbb{E}(F) = \mu/n \approx e^{-m/n}$. ?` Cu\'an cerca est\'a Z de $\mu$?. Se debe notar que las variables $Z_i$ no son independientes, por lo que no se puede aplicar la desigualdad de Hoeffding. En este caso procedemos de otra forma:

\vspace{0.2cm}


Definamos las variables $X_1, \dots, X_m$ donde $X_s = i$ si la pelota $s$ cae en el recipiente $i$. Entonces $Z = g(X_1,\dots, X_m)$. Si movemos una pelota a un recipiente diferente, entonces $Z$ puede cambiar a lo m\'as $1$. As\'i del Teorema de McDiarmind con $c_i = 1$ tenemos:

\vspace{0.3cm}

\[
\mathbb{P}(\vert Z - \mu\vert  > t) \leq 2e^{-2t^2/m}.
\]

\vspace{0.3cm}

Como las fracci\'on de recipientes vacios es $F=Z/m$ con media $\theta = \mu/n$. Tenemos:


\[
\mathbb{P}(\vert F - \theta\vert > t) = \mathbb{P}(\vert Z - \mu\vert> nt) \leq 2e^{-2n^2t^2/m}.
\]

\end{ejemplo}



\subsection{Cotas para los valores esperados}


\begin{teo}
\normalfont \textbf{Desigualdad de Cauchy -Schwartz} Si $X$ y $Y$ tienen varianza finita, entonces:

\[
\mathbb{E}\vert XY\vert \leq \sqrt{\mathbb{E}(X^2)\mathbb{E}(Y^2)}.
\]

\vspace{0.2cm}


La desigualdad de Cauchy-Schwarz puede ser escrita como:

\[
\text{Cov}^2(X,Y) \leq \delta_{X}^{2}\delta_{Y}^{2}.
\]

\end{teo}

\vspace{0.3cm}

\begin{teo}
\normalfont\textbf{Desigualdad de Jensen} Si $g$ es convexa, entonces:
\[
\mathbb{E}g(X) \geq g(\mathbb{E}X)
\]


\vspace{0.2cm}

Si $g$ es c\'oncava

\[
\mathbb{E}g(X) \leq g(\mathbb{E}X).
\]


\vspace{0.2cm}

En efecto, sea $L(x)= a +bx$ una l\'inea tangente a $g(x)$ en el punto $\mathbb{E}(X)$. Desde que $g$ es convexa, est\'a por encima de la l\'inea $L(X)$. As\'i:

\vspace{0.2cm}

\[
\mathbb{E}g(X) \geq \mathbb{E}L(X) = \mathbb{E}(a +bX) = a + b\mathbb{E}(X) = L(\mathbb{E}(X)) = g(\mathbb{E}X).
\]

\vspace{0.2cm}

De la desigualdad de Jensen, se cumple $\mathbb{E}(X^2) \geq (\mathbb{E}X)^2$ y si $X$ es positiva, entonces $\mathbb{E}(1/X) \geq 1/\mathbb{E}(X)$. Desde que $\log$ es c\'oncava, $\mathbb{E}(\log X) \leq \log\mathbb{E}(X)$.

\end{teo}

\vspace{0.3cm}

\section{La ley de promedios}

Si lanzamos un dado $5$ millones de veces y se mantiene un registro de los datos. El promedio de los n\'umeros  los cuales fueron lanzados es $3.500867$. Desde que la media de cada lanzamiento es $\frac{1}{6}(1 + 2 + \cdots +6) = 3\frac{1}{2}$, este resultado muestra  que para un $x_i$ el $i$ \'esimo lanzamiento, el promedio definido como:

\vspace{0.2cm}

\[
a_n = \dfrac{1}{n}(x_i + x_2 + \cdots + x_n)
\]

\vspace{0.2cm}

se acerca a la media $3\dfrac{1}{2}$, cuando $n \rightarrow \infty$.

\vspace{0.2cm}

En general si tenemos una secuencia $X_1, X_2, \dots$ de variables aleatorias identicamente distribuidas e independientes cada una teniendo una media $\mu$, deberiamos probar que el promedio:

\[
\overline{X}_n= \dfrac{1}{n}(X_1 + X_2 +\cdots +X_n)
\]

\vspace{0.2cm}

converge cuando $n \rightarrow \infty$ a la media $\mu$.

\vspace{0.2cm}
\begin{defi}
\normalfont Decimos que la secuencia $X_1, X_2, \dots$ de variables aleatorias \texttt{converge en media  cuadrada a la variable X} si se cumple:

\vspace{0.2cm}

\[
\mathbb{E}([X_n - X]^2)\rightarrow 0 \ \ \text{cuando}\ \ n\rightarrow \infty
\]

\vspace{0.2cm}

y se escribe \texttt{$X_n \rightarrow x$ en media cuadrada cuando $n \rightarrow \infty$}.
\end{defi}

\vspace{0.2cm}

\begin{ejemplo}
\normalfont Sea $X_n$ una secuencia de variables aleatorias discretas con una funci\'on de masa de probabilidad:

\vspace{0.2cm}

\[
\mathbb{P}(X_n = 1) =  \dfrac{1}{n},  \ \ \ \ \mathbb{P}(X_n = 2) = 1 - \dfrac{1}{n}.
\]

\vspace{0.2cm}

Entonces $X_n$ converge a la variable aleatoria $2$ en media cuadrada cuando $n \rightarrow \infty$, desde que

\vspace{0.2cm}

\begin{align*}
\mathbb{E}([X_n - 2]^2) = (1 -2)\dfrac{1}{n} + (2 -2)^2\Bigl(1 - \dfrac{1}{n}\Bigr)\\
= \dfrac{1}{n} \rightarrow 0 \ \ \ \text{cuando}\ \ n \rightarrow \infty.
\end{align*}
\end{ejemplo}

\vspace{0.3cm}

\subsection{Primera versi\'on de la Ley de los Grandes N\'umeros}

\begin{teo}
\normalfont  Sea una secuencia $X_1, X_2, \dots $ una secuencia de variables independientes cada una con media $\mu$ y varianza $\sigma^2$. El promedio de los primeras $n$ variables aleatorias $X_i$ satisfacen cuando $n \rightarrow \infty$ que:

\vspace{0.2cm}

\[
\dfrac{1}{n}(X_1 +X_2 + \cdots + X_n) \rightarrow \mu \ \ \ \text{en media cuadrada}.
\]


\vspace{0.2cm}

Sea $S_n = X_1 + X_2 + \cdots X_n$, la n-\'esima suma parcial de los $X_i$. 

\vspace{0.2cm}

Entonces:

\vspace{0.2cm}

\[
\mathbb{E}\Bigl(\dfrac{1}{n}S_n\Bigr) = \dfrac{1}{n}\mathbb{E}(X_1 +X_2 + \cdots + X_n) = \dfrac{1}{n}n\mu = \mu.
\]

\vspace{0.2cm}

y as\'i:

\vspace{0.2cm}

\begin{align*}
\mathbb{E}\Bigl(\Bigl[\dfrac{1}{n}S_n - \mu \Bigr]^2\Bigr) &= \text{Var}\Bigl(\dfrac{1}{n}S_n\Bigr)\\
  &= \dfrac{1}{n^2}\text{Var}(X_1 +X_2 + \cdots + X_n)\\
  &= \dfrac{1}{n^2}(\text{Var}X_1 + \cdots  + \text{Var}X_n)\\
  &=\dfrac{1}n{n^2}\sigma^2 = \dfrac{1}{n}\sigma^2 \rightarrow 0 \ \ \ \text{cuando}\ \ n \rightarrow \infty
\end{align*}

\vspace{0.2cm}

y as\'i, $n^{-1}S_n \rightarrow \mu$ en media cuadr\'atica cuando $n \rightarrow \infty$.
\end{teo}

\begin{defi}
\normalfont Decimos que la secuencia de variables aleatoria $X_1, X_2, \dots$ de variables aleatorias \texttt{converge en probabilidad } a $X$ si,

\vspace{0.2cm}

\[
\texttt{Para todo} \ \ \epsilon >0 \ \ \ \mathbb{P}(\vert X_n - X\vert > \epsilon) \rightarrow \ \ \text{cuando}\ \ n \rightarrow \infty
\]

\vspace{0.2cm}

Si esto se cumple, escribimos \texttt{$X_n \rightarrow X$ en probabilidad cuando $n \rightarrow \infty$ }

\end{defi}

\vspace{0.3cm}

\begin{teo}
\normalfont Si $X_1, X_2, \dots $ es una secuencia de variables aleatorias y $X_n \rightarrow X$ en media cuadrada cuando $n \rightarrow \infty$ entonces $X_n \rightarrow X$ en probabilidad.

\vspace{0.2cm} 

Para hacer la prueba de este teorema, escribamos la \texttt{la desigualdad de Chebyshev} de la siguiente manera

\vspace{0.2cm}

Si $Y$ es una variable aleatoria y $\mathbb{E}(Y^2) < \infty$, entonces:

\vspace{0.2cm}

\[
\mathbb{P}(\vert Y \vert \geq t) \leq \dfrac{1}{t^2}\mathbb{E}(Y^2)\ \ \ \text{para}\ \ \ t >0
\]


Luego aplicando la desigualdad de Chebyshev a la variable aleatoria $Y = X_n - X$ para encontrar que:

\vspace{0.2cm}

\[
\mathbb{P}(\vert X_n -X\vert > \epsilon) \leq \dfrac{1}{\epsilon^2}\mathbb{E}([X_n -X]^2)\ \ \ \epsilon > 0.
\]

\vspace{0.2cm}

Si $X_n \rightarrow X$ en media cuadrada cuando $n \rightarrow \infty$, el lado derecho de la desigualdad tiende a $0$ cuando $n \rightarrow \infty$ y as\'i tiende a $0$ para todo $\epsilon > 0$, como es requerido. El caso contrario no se cumple.
\end{teo}

\vspace{0.3cm}

\subsection{Ley D\'ebil de los Grandes N\'umeros}
\begin{teo}
\normalfont Sea $X_1, X_2, \dots$ una secuencia de variables aleatorias, cada una con media $\mu$ y varianza $\sigma^2$. El promedio de los primeros $n$ de los $X_i$ satisface, cuando $n \rightarrow \infty$,

\vspace{0.2cm}

\[
\dfrac{1}{n}(X_1 +X_2 + \cdots + X_n) \rightarrow \mu \ \ \ \text{en probabilidad}.
\]
\end{teo}
\vspace{0.2cm}

\begin{ejemplo}
\normalfont Sea $X_n$ una secuencia de variables aleatorias que convergen en probabilidad, mostremos que no converge  en media cuadrada. Si las variables aleatorias tienen  una funci\'on de masa de probabilidad:

\vspace{0.2cm}

\[
\mathbb{P}(X_n = 0) =  1 - \dfrac{1}{n},  \ \ \ \ \mathbb{P}(X_n = n) = \dfrac{1}{n}.
\]


\vspace{0.2cm}

Entonces, para $\epsilon > 0$ y para un $n$ dado, tenemos: 

\vspace{0.2cm}


\[
\mathbb{P}(\vert X_n\vert > \epsilon) = \mathbb{P}(X_n = n) = \dfrac{1}{n} \rightarrow 0 \ \ \ \text{cuando}\ \ n \rightarrow \infty
\]

\vspace{0.2cm}

dando que $X_n \rightarrow  0$ en probabilidad. Por otro lado, 

\vspace{0.2cm}

\begin{align*}
\mathbb{E}([X_n - 0]^{2}) = \mathbb{E}(X_n^2) = 0\Bigl(1 -\dfrac{1}{n}\Bigr) + n^2\dfrac{1}{n}\\
= n \rightarrow \infty \ \  \text{cuando} \ \ n \rightarrow \infty.
\end{align*}

\vspace{0.2cm}

as\'i $X_n$ no converge a $0$ en media cuadrada.
\end{ejemplo}

\vspace{0.2cm}

\begin{ejemplo}
\normalfont Considere el lanzamiento de  una moneda donde la probabilidad de obtener cara es $p$. Sea $X_i$  el resultado de un lanzamiento $(0-1)$. Por lo tanto, $p = \mathbb{P}(X_i = 1) = \mathbb{E}(X_i)$. La fracci\'on de caras  despu\'es de $n$ lanzamientos  es  $\overline{X}_n$. De acuerdo con la ley de los grandes n\'umeros, $\overline{X}_n$ converge en probabilidad a $p$. Esto no quiere decir que $\overline{X}_n$ sea num\'ericamente igual $p$. Esto significa que, cuando $n$ es grande, la distribuci\'on de $\overline{X}_n$ est\'a  \texttt{concentrado} alrededor $p$.

\vspace{0.2cm}

Supongamos que $p = 1/2$, encontremos el valor de $n$ para que $\mathbb{P}(.4\leq \overline{X}_n \leq .6) \geq .7$.

\vspace{0.2cm}

Para ello tenemos que $\mathbb{P}(\overline{X}_n) = p = 1/2$ y $\text{Var}(\overline{X}_n) = \sigma^2/n = p(1 -p)/n = 1/n$. Por la desigualdad de Chebyshev, se tiene:

\vspace{0.2cm}

\begin{align*}
\mathbb{P}(.4\leq \overline{X}_n \leq .6) = \mathbb{P}(\vert \overline{X}_n - \mu) \leq .1)\\
= 1 - \mathbb{P}(\vert \overline{X}_n - \mu\vert > .1)\\
\geq 1 - \dfrac{1}{4n(.1)^2} = 1 - \frac{25}{n}
\end{align*}

\vspace{0.2cm}

Esta expresi\'on es mayor que $.7$ si $n = 84$.
\end{ejemplo}

\vspace{0.3cm}

\subsection{Teorema del L\'imite Central}

\begin{teo}
\normalfont Sea $X_1, X_2, \dots $ variables aleatorias identicamente distribuidas e independientes cada una con media $\mu$ y varianza distinta de cero $\sigma^2$. Por la ley de los grandes n\'umeros, la suma $S_n = X_1 +X_2+ \cdots +X_n$ es casi tan grande que $n\mu$ para valores de $n$ muy grandes. Un siguiente paso es determinar el orden de la diferencia $S_n - n\mu$, que resulta ser de orden $\sqrt{n}$.

\vspace{0.2cm}

Se define la versi\'on est\'andar de $S_n$

\vspace{0.2cm}

\[
Z_n = \dfrac{S_n -\mathbb{E}(S_n)}{\sqrt{\text{Var}(S_n)}}
\]

\vspace{0.2cm}

Esta es una funci\'on lineal $Z_n = a_nS_n + b_n$ de $S_n$, donde $a_n$ y $b_n$ tienen que ser elegida de manera que $\mathbb{E}(Z_n) = 0$ y $\text{Var}(Z_n) = 1$. Adem\'as:

\vspace{0.2cm}

\begin{align*}
\mathbb{E}(S_n ) &= \mathbb{E}(X_1) + \mathbb{E}(X_2) + \cdots + \mathbb{E}(X_n )\\
&= n\mu
\end{align*}


Tambi\'en, 

\vspace{0.2cm}

\begin{align*}
\text{Var}(S_n) &= \text{Var}(X_1) + \text{Var}(X_2) + \cdots +\text{Var}(X_n)\\
&= n\sigma^2
\end{align*}

y as\'i,

\vspace{0.2cm}

\[
Z_n = \dfrac{S_n - n\mu}{\sigma\sqrt{n}}
\]
\end{teo}


\vspace{0.5cm}


Muchas de las propiedades de $Z_n$ se pueden sintetizar en el teorema de l\'imite central:

\vspace{0.3cm}

Sean $X_1, X_2, \dots $ variables aleatorias identicamente distribuidas e independiente, cada una con media $\mu$ y varianza distinta de cero $\sigma^2$. La versi\'on est\'andar: 

\vspace{0.2cm}

\[
Z_n = \dfrac{S_n - n\mu}{\sigma\sqrt{n}}
\]

\vspace{0.2cm}

de la suma $S_n = X_1 +X_2 + \cdots + X_n$, satisface cuando $n \rightarrow \infty $:

\vspace{0.2cm}

\[
\mathbb{P}(Z_n \leq x) \rightarrow \bigintss_{-\infty}^{x}\dfrac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}u^2}du = \Phi(x)\ \ \ \text{para}\ \ x\in \mathbb{R}.
\]

\vspace{0.2cm}

El lado derecho de la \'ultima de la ecuaci\'on es s\'olo la funci\'on de distribuci\'on de la distribuci\'on normal con media $0$ y varianza $1$, as\'i que esta expresi\'on se puede escribir como:

\vspace{0.2cm}

\[
\mathbb{P}(Z_n \leq x) \rightarrow \mathbb{P}(Y \leq x) \ \ \ \text{para}\ \ x \in \mathbb{R}.
\]


\vspace{0.2cm}


donde $Y$ es la variable aleatoria con esta distribuci\'on normal est\'andar. Tambi\'en podemos  escribir el teorema  de la siguiente forma: 

\vspace{0.2cm}

\[
\lim_{n \rightarrow \infty}\mathbb{P}(Z_n \leq x) = \Phi(x)\ \ \text{para cada $x$}.
\]

\vspace{0.2cm}


El teorema del l\'imite central, nos permite calcular probabilidades relacionadas a $Z_n$ como si $Z_n$ fuese normal. Desde que la normalidad es preservada bajo transformaciones lineales, esto es equivalente  a tratar $S_n$ como una distribuci\'on normal con media $n\mu$ y varianza $n\sigma^2$. 

\vspace{0.2cm}


Sea $S_n = X_1 +X_2+ \cdots +X_n$ donde las $X_i$ son variables aleatorias id\'enticamente distribuidas e independientes cada una con media $\mu$ y varianza distinta de cero $\sigma^2$. Si $n$ es muy grande, la probabilidad $\mathbb{P}(S_n \leq c)$ puede ser aproximada considerando $S_n$ como si fuese normal de acuerdo  al siguiente procedimiento:

\vspace{0.2cm}

\begin{itemize}
\item Calculamos la media $n\mu$ y la varianza $n\sigma^2$ de $S_n$.
\item Calculamos el valor normalizado $z = (c - n\mu)/\sigma\sqrt{n}$.
\item Usamos la aproximaci\'on, 

\[
\mathbb{P}(S_n \leq c) \approx \Phi(z),
\]

\vspace{0.2cm}

donde $\Phi(z)$ es  disponible desde la tabla de funci\'on densidad acumulativa   de la distribuci\'on est\'andar. 
\end{itemize}


\vspace{0.2cm}

\begin{ejemplo}
\normalfont  Cargamos en un avi\'on $100$ paquetes cuyos pesos son  variables aleatorias independientes  que se distribuyen uniformemente entre $5$ y $50$ libras. ?`Cu\'al es la probabilidad de que el peso total es superior a 3000 libras?.

\vspace{0.2cm}

No es f\'acil calcular la funci\'on densidad acumulativa  del peso total y la probabilidad deseada, pero una  respuesta aproximada se puede obtener usando el teorema del l\'imite central:

\vspace{0.2cm}

Calculemos $\mathbb{P}(S_{100} > 3000)$, donde $S_{100}$ es la suma de los pesos de los $100$ paquetes. En este caso la media y la varianza de los pesos de un \'unico paquete es: 

\vspace{0.2cm}

\[
\mu = \dfrac{5 + 50}{2} = 27.5 \ \ \ \ \sigma^{2} = \dfrac{(50 -5)^2}{12} = 168.75.
\]


\vspace{0.3cm}

basados en la f\'ormulas para la media y la varianza de la funci\'on densidad de una distribuci\'on  uniforme, calculemos el valor normalizado :

\vspace{0.2cm}

\[
z = \dfrac{3000 - 100\cdot27.5}{\sqrt{168.75 -100}} = \dfrac{250}{129.9} = 1.92
\]

\vspace{0.3cm}

y usando  las tablas normal est\'andar para obtener la aproximaci\'on tenemos:

\vspace{0.2cm}

\[
\mathbb{P}(S_{100} \leq 3000) \approx \Phi(1.92) = 0.9726.
\]

\vspace{0.2cm}

y as\'i la probabilidad pedida es:

\vspace{0.2cm}

\[
\mathbb{P}(S_{100} > 3000) = 1 - \mathbb{P}(S_{100} \leq 3000) \approx 1 - 0.97626 = 0.0274.
\]

\end{ejemplo}


\subsection{Distribuciones que no cumplen el teorema del l\'imite central}

No todas las distribuciones obedecen al teorema del l\'imite central. Un ejemplo es la variable aleatoria de Cauchy, que tiene como  funci\'on  densidad de probabilidad:

\[
f(x) = \frac{1}{\pi(1 + x)^2}.
\]

La esperanza y todos los momentos superiores de una variable aleatoria de Cauchy no est\'an definidos. En particular, no tiene una varianza finita y por lo tanto no satisface las condiciones del teorema del l\'imite central.
\end{document}

