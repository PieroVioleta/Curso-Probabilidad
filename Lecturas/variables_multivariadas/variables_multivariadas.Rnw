\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{bigints}
\usepackage[latin1]{inputenc}
\usepackage{geometry}
\usepackage{amsmath} 
\usepackage{mathtools}
%\usepackage[spanish]{babel}
\usepackage{latexsym}
\geometry{verbose,tmargin=1.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage{graphicx}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bb}[1]{\textbf{#1}}
\DeclareMathOperator{\rank}{\textbf{rango}}
\DeclareMathOperator{\proy}{\textbf{proy}}
\DeclareMathOperator{\nulll}{\textbf{nul}}
\DeclareMathOperator{\diag}{\textbf{diag}}
\DeclareMathOperator{\col}{\textbf{col}}
\DeclareMathOperator{\fila}{\textbf{fila}}
\DeclareMathOperator{\dimm}{dim}
\DeclareMathOperator{\Traz}{Tr}
%\theoremstyle{definition}
\everymath{\displaystyle}
\newtheorem{ejemplo}{{Ejemplo }}[section]
\newtheorem{teo}{{Teorema}}[section]
\newtheorem{defi}{{Definici\'on}}[section]
\newtheorem{pros}{{Proposici\'on}}[section]
\newtheorem{cor}{{Corolario}}[section]
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(ggplot2)
library(grid)
#library(animation)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@

%\title{\underline{\textbf{Notas de mat\'ematica}}}
%\date{}
%\maketitle
\hspace*{0.5\linewidth}
\begin{minipage}{0.6\linewidth}
Curso: Introducci\'on a la Probabilidad y Estad\'istica CM -274\par
Variables aleatorias multivariadas (1)\par
\end{minipage}


\vspace {0.5cm}


\section{Variables aleatorias multivariadas}

\vspace{0.5cm}


Las relaciones que pueden existir entre dos o m\'as variables aleatorias, suele ser un problema de modelado m\'as com\'un que uno que involucra una \'unica variable aleatoria. Por ejemplo, podemos estar interesados  en relacionar la variable aleatoria $X$ que mide la intensidad de una se\~nal de tel\'efono celular en alg\'un lugar y la variable aleatoria $Y$ que mide la distancia de ese lugar hacia el local  de cobertura m\'as cercano. O podemos estar interesados en relacionar una variable aleatoria $X$ que define el proceso de llegada de los pacientes al consultorio de un  m\'edico y la variable aleatoria $Y$ que mide el tiempo que pasan los clientes en la sala de espera. Como ejemplo final, podemos estar interesados, en examinar la situaci\'on en la que los trabajos que llegan  a un f\'abrica en el que las m\'aquinas est\'an sujetas a fallas. Se puede usar una primera variable aleatoria para indicar el n\'umero de trabajos presentes y se puede usar un segunda variable para representar el n\'umero de m\'aquinas de trabajo.

\vspace{0.3cm}

Las sumas, las diferencias y los productos de las variables aleatorias suelen ser importantes, al igual que las relaciones como el m\'aximo y el m\'inimo. Considere la posibilidad de ejecutar un programa de computadora que consta de diferentes secciones cuyo tiempo para completar depende de la cantidad y el tipo de datos proporcionados.

\vspace{0.2cm}


De ello se deduce que el tiempo necesario para ejecutar todo el programa depende tambi\'en de los datos. El tiempo de ejecuci\'on de la i-\'esima secci\'on puede ser representado por una variable aleatoria $X_i$. Sea la variable aleatoria $X$ la que  represente el tiempo necesario para ejecutar todo el programa. Si las secciones se ejecutan una tras otra, entonces $X$ est\'a dado por $X = \sum_iX_i$. Si las secciones se ejecutan en paralelo, entonces $X = \max\{X_i,  i = 1, 2,\dots \}$.

\subsection{Funciones de masa de probabilidad conjunta}


La funci\'on de masa de probabilidad para una variable aleatoria $X$, es denotada por $p_X(x)$ y es la probabilidad que $X$ toma el valor de $x$, esto es, $p_X(x)$ es la probabilidad de los eventos que consisten de todas las salidas que son mapeadas en el valor $x$. Si $X$ y $Y$ son variables aleatorias, una salida es un punto  en plano $(x,y)$ y un evento es un subconjunto de esos puntos del plano: $[X = x, Y = y]$ es el evento que consiste de todas las salidas para el cual $X$ es mapeada a $x$ y $Y$ es mapeada a $y$. La \texttt{funci\'on de masa de probabilidad conjunta} denotada como $p_{X,Y}$  es igual a la probabilidad del evento $[X = x, Y = y]$ y es la funci\'on  $p_{X,Y} : \mathbb{R}^2 \rightarrow [0,1]$ definida por


\vspace{0.3cm}

\[
p_{X,Y}(x,y) = \mathbb{P}(\omega \in \Omega :X(\omega) =x , Y(\omega)=y)
\]

\vspace{0.3cm}

o de forma abreviada :

\vspace{0.3cm}

\[
p_{X,Y}(x,y) = \mathbb{P}(X=x , Y=y)
\]


\vspace{0.2cm}

La funci\'on de masa de probabilidad conjunta de dos variables aleatorias discretas puede representarse convenientemente en forma tabular. Por ejemplo, podemos colocar valores realizables de $X$ en la parte superior de una tabla y los de $Y$ en el lado izquierdo. El elemento de la tabla correspondiente a la columna $X = x$ y la fila $Y = y$ denota la probabilidad del evento $[X = x, Y =y]$,

\vspace{0.3cm}


\begin{ejemplo}
\normalfont Sea $X$ una variable aleatoria que tiene los valores en $\{1, 2, 3, 4, 5 \}$ y $Y$ una variable aleatoria con valores en $\{-1, 0, 1, 2\}$ y sea la funci\'on de masa de probabilidad conjunta dada por $p_{X,Y}(x, y) = \alpha$, para todos los posibles valores de $x$ e $y$. Para calcular los valores de $\alpha$, observamos que s\'olo $20$ punto en el plano $(x, y)$ tiene probabilidad positiva y adem\'as la probabilidad de cada uno es la misma. Esto nos d\'a $\alpha = 1/20 = 0.05$. Las siguientes tablas muestran la funci\'on de masa de probabilidad conjunta de $X$ y $Y$.

\vspace{0.2cm}

\begin{table}[h]
\centering
\begin{tabular}{l|cc  ccc}
\hline
      & x = 1 & x = 2 & x=3 & x =4 & x =5 \\
      \hline
y = -1 & 1/20   &1/20 &1/20 & 1/20 & 1/20 \\
y =  0 & 1/20   &1/20 &1/20 & 1/20 & 1/20  \\
y =  1 & 1/20   &1/20 &1/20 & 1/20 & 1/20 \\
y =  2 & 1/20   &1/20 &1/20 & 1/20 & 1/20 \\
\hline
\end{tabular}
\end{table}
\end{ejemplo}

\vspace{0.2cm}

Cada evento $A$ es un subconjunto de puntos en el plano $(x, y)$, por lo que podemos escribir como:

\vspace{0.2cm}

\[
\mathbb{P}(A) = \sum_{(x,y )\in A}p_{X,Y}(x, y).
\]

Donde la suma es sobre todos los puntos $(x,y) \in A$. La siguiente figura, muestra dos eventos $A = [X + Y < 4]$ y $B = [\vert X \vert + \vert Y \vert \leq 3]$, donde todos los posibles valores de $X$ e $Y$ est\'an marcados por puntos.


\vspace{0.2cm}

\begin{figure}[h]
\centering
\includegraphics[scale=.65]{l1.png}
\end{figure}

\vspace{0.2cm}

\begin{ejemplo}
\normalfont Sean $X$ e $Y$ variables aleatorias continuas cuya funci\'on de masa de probabilidad conjunta es dada por el anterior ejemplo. Sea $A$ el evento que $X$ no tiene valores mayores que $Y$, escrito como $[X \leq Y]$. Los puntos en el plano, teniendo una probabilidad positiva, para el cual, el valor de $X$ es menor o igual al valor de $Y$ son $(1,1), (1,2) $ y $(2,2)$. Por tanto la probabilidad de este evento es dado por $\mathbb{P}(A) = \mathbb{P}(X \leq Y) = 3/20$.

\vspace{0.2cm}

Sea ahora el evento $[XY = 0]$. Este evento contiene, cinco salidas: $(1,0), (2,0), (3,0), (4,0)$ y $(5,0)$ y as\'i $\mathbb{P}(B) = \mathbb{P}(XY = 0) = 5/20$.
\end{ejemplo}


\vspace{0.2cm}

La funci\'on de masa de probabilidad conjunta, especifica la probabilidad de eventos conjuntos, denotados por $[X = x, Y = y]$. Es posible considerar la probabilidad de eventos que se refiere a una de las dos variables aleatorias, tales como $[X = x]$. En efecto considerando el ejemplo (1.1), el evento $[X = 2]$ consiste de cuatro salidas $(2, -1), (2, 0), (2,1)$ y $(2,2)$ y tienen una probabilidad  igual a $4/20$. En este caso, usamos la funci\'on de masa de probabilidad conjunta de las dos variables aleatorias para construir la funci\'on de masa de probabilidad para cada una de las variables aleatorias. Esas funciones son las \texttt{funciones de masa probabilidad marginal}. La funci\'on de masa de probabilidad marginal de $X$ es obtenida como:

\vspace{0.2cm}

\begin{equation}
p_X(x) = \mathbb{P}(X = x) = \sum_k\mathbb{P}(X =x, Y = y_k) = \sum_{k}p_{X,Y}(x, y_k).
\end{equation}

\vspace{0.2cm}

Y es una consecuencia directa, del hecho que para un evento $A$, $\mathbb{P}(A) = \sum_{(x,y) \in A}p_{X,Y}(x, y)$.

La distribuci\'on marginal de $Y$ es obtenida de manera similar.


\vspace{0.2cm}

\begin{defi}
\normalfont Las variables aleatorias $X$ e $Y$ se dicen que son independientes si se cumple :

\vspace{0.2cm}

\[
\mathbb{P}(X = x_i, Y = y_i) = \mathbb{P}(X = x_i)\mathbb{P}(Y = y_i)
\]

\vspace{0.2cm}

Para todo $x_i$ y $y_i$. En otras palabras, $X$ e $Y$ son independientes si su funci\'on de masa conjunta, se factoriza en el producto de sus funciones de masa de probabilidad individualmente.
\end{defi}

\vspace{0.2cm}

\begin{ejemplo}
\normalfont Sea una distribuci\'on para dos variables aleatorias $X$ y $Y$ que  toman los valores $0$ - $1 $:

\vspace{0.2cm}

\begin{table}[h]
\centering
\begin{tabular}{c|cc|c}
      & Y = 0 & Y = 1 &     \\
      \hline
X = 0 & 1/9   & 2/9   & 1/3 \\
X = 1 & 2/9   & 4/9   & 2/3 \\
\hline
      & 1/3   & 2/3   &  1  
\end{tabular}
\end{table}

\vspace{0.2cm}

Entonces  $p_{X,Y}(1,1) = \mathbb{P}(X = 1,y =1) = 4/9$.
\end{ejemplo}


\vspace{0.2cm}

\begin{ejemplo}
\normalfont Sean $X$ e $Y$ dos variables aleatorias discretas con una funci\'on de masa de probabilidad conjunta :

\vspace{0.2cm}

\begin{table}[h]
\centering
\begin{tabular}{l|cc c}
  \hline
      & X = -1 & X = 0 & X=1     \\
      \hline
Y = -1 & 1/12   &3/12    & 1/12 \\
Y = 0  & 1/12   &0/12   & 1/12 \\
Y = 1  & 1/12   &3/12    & 1/12 \\
\hline
\end{tabular}
\end{table}

\vspace{0.2cm}

As\'i, ambas variables aleatorias $X$ y $Y$ toman los valores, desde el conjunto $\{-1, 0, 1 \}$. La tabla indica que $\mathbb{P}(X= 0, Y = 0) = 0$, que $\mathbb{P}(X = 0, Y =1) = 1/4$ y as\'i sucesivamente. La funci\'on de masa de probabilidad marginal de $X$, $p_X(x)$ es obtenida sumando las entradas en cada columna  de la tabla, la de  $Y$, $p_Y(y)$ se encuentra sumando las entradas en cada fila de la tabla. Por lo que se tiene la siguiente tabla:

\vspace{0.2cm}

\begin{table}[h]
\centering
\begin{tabular}{l|cc  cc}
  \hline
      & X = -1 & X = 0 & X=1 & $p_Y(y)$ \\
      \hline
Y = -1 & 1/12   &3/12    & 1/12 & 5/12\\
Y = 0  & 1/12   &0/12   & 1/12  &1/6\\
Y = 1  & 1/12   &3/12    & 1/12 & 5/12\\
$p_X(x)$ & 1/4    &1/2     &1/4 & 1 \\
\hline
\end{tabular}
\end{table}

\vspace{0.2cm}

En este ejemplo, $X$ e $Y$ no son independientes, desde que :

\vspace{0.2cm}

\[
1/12 = \mathbb{P}(X = 1, Y=1) \neq \mathbb{P}(X =1)\mathbb{P}(Y =1) = 3/12 \times 5/12.
\]
\end{ejemplo}

\vspace{0.2cm}


De manera similar, se aplica a $X = (X_1, X_2, \dots,X_n)$ un vector de variables aleatorias discretas. Para este caso la funci\'on de masa de probabilidad conjunta  de $X$ es definido como la funci\'on $p_X$ definida por:

\vspace{0.2cm}

\[
p_{X}(x) = \mathbb{P}(X_1 = x_1, X_2 = x_2, \dots, X_n= x_n)
\]

\vspace{0.2cm}

para $x = (x_1, x_2, \dots, x_n) \in \mathbb{R}^n$.

\subsection{Funci\'on de distribuci\'on  acumulativa conjunta}

\vspace{0.2cm}

La funci\'on de distribuci\'on acumulativa conjunta de dos variables $X$ e $Y$ es dada por:

\vspace{0.2cm}

\[
F_X,Y(x,y) = \mathbb{P}(X \leq x, Y \leq y);\ \ -\infty < x < \infty, \ -\infty < y < \infty.
\]

\vspace{0.2cm}

Para ser un CDF de buena fe, una funci\'on debe poseer las siguientes propiedades, similares a las encontradas en el CDF de una sola variable aleatoria:

\begin{itemize}
\item $0 \leq F_{X,Y}(x,y) \leq 1$ para $-\infty < x < \infty$, \ $-\infty < y < \infty$, desde  que $F_{X,Y}(x,y)$ es una probabilidad.
\item $F_{X,Y}(x, \infty) = F_{X,Y}(-\infty, y) = 0$ \\
$F_{X,Y}(-\infty, -\infty) = 0$, $F_{X,Y}(+\infty, +\infty) = 1$.
\item $F_{X,Y}$ debe ser una funci\'on no decreciente de ambos $x$ e $y$, es decir:

\[
F_{X,Y}(x_1, y_1) \leq F_{X,Y}(x_2, y_2)\ \text{si}\ x_1 \leq x_2\ \ \text{si}\ \ y_1 \leq y_2.
\]
\end{itemize}

\vspace{0.2cm}

\begin{ejemplo}
\normalfont La funci\'on distribuci\'on acumulativa de una variable aleatoria $X$ dado en forma tabular, es dada por:

\vspace{0.2cm}

\begin{table}[h]
\centering
\begin{tabular}{l|c c c c c}
  \hline
      & $-2 < x$ & $-2 \leq x < -1$ & $-1 \leq x < 0$ & $0 \leq x < 1$ & $1 \leq x$ \\
      \hline
$F_x(x)$  & 0   &1/8    & 1/4 & 1/2 & 1 \\
\hline
\end{tabular}
\end{table}


\vspace{0.2cm}

Esto nos dice, por ejemplo que $\mathbb{P}(X < 1.0) = 1/2$. Tambi\'en $\mathbb{P}(X \leq 0.5) = 1/2, \mathbb{P}(X \leq 0.0) = 1/2$ y as\'i. De manera similar $\mathbb{P}(X <  -1.0) = 1/8,  \mathbb{P}(X \leq -1.5) = 1/8$ . Usando el mismo enfoque, podemos encontrar la funci\'on  de distribuci\'on  acumulutativa conjunta de dos variables aleatorias discretas $X$ e $Y$ en forma tabular. Un ejemplo es el siguiente:

\vspace{0.2cm}

\begin{table}[h]
\centering
\begin{tabular}{c|cc  ccc c}
      \hline
$y \geq 2$       & 0   &1/8  & 1/4 & 1/2 & 1 \\
$0 \leq y < 2$   & 0   &3/32 & 3/16 &3/8  & 3/4\\
$-2 \leq y < 0$  & 0  & 1/32 & 1/16 & 1/8 & 1/4 \\
$y< 2$           & 0  & 0   &0 &  0 & 0 \\
\hline
$F_{X,Y}(x,y) $& $-2 < x$ & $-2 \leq x < -1$ & $-1 \leq x < 0$ & $0 \leq x < 1$ & $1 \leq x$ 
\end{tabular}
\end{table}

\vspace{0.2cm}

De esta tabla, podemos encontrar por ejemplo que $\mathbb{P}(X < 1, Y < 0) = 1/8$, que $\mathbb{P}(X \leq 0.5, Y < 0) = 1/8 = \mathbb{P}(X \leq 0.0, Y < 0)$, que $\mathbb{P}(X \leq -1.3, Y \leq  1.3) = 3/32$, que $\mathbb{P}(X \geq 1, Y < 1) = 3/4$ y as\'i. 
\end{ejemplo}

\vspace{0.2cm}

Se observa que las condiciones detalladas anteriormente para que una funci\'on sea un CDF de buena fe se mantienen en este ejemplo.

\vspace{0.2cm}

Mientras que una sola variable aleatoria mapea los resultados en puntos de la l\'inea real, la variable aleatoria conjunta mapea los resultados en puntos en un espacio de mayor dimensi\'on. Por ejemplo, dos variables aleatorias asignan resultados en el plano $(x, y)$, tres  variables aleatorias mapean los resultados  en el espacio tridimensional $(x, y, z)$  y as\'i sucesivamente. Para una sola variable aleatoria, el valor de la funci\'on de distribuci\'on acumulativa en un punto $x$ es igual a la probabilidad del evento que contiene todos los resultados que son mapeadas en puntos  menores o iguales a $x$. 

\vspace{0.2cm}


El valor de la distribuci\'on acumulativa de dos variables aleatorias $X$ e $Y$ en el punto $(x, y)$ es la probabilidad del evento que contiene todos los resultados que se asignan en el \'area infinita que se encuentra a la izquierda de una l\'inea vertical a trav\'es de $X = x$  y por debajo de la l\'inea horizontal a trav\'es de $Y = y$.


\vspace{0.2cm}

Puede observarse de la siguiente  figura, que $F_{X,Y}(a ,b) = \mathbb{P}(X \leq b, Y \leq d)$ es igual a la probabilidad del evento que contiene todas las salidas dentro de las \'areas sombreadas (sombreado ligero y fuerte), mientras que $F_{X,Y}(a, c) = \mathbb{P}(X \leq a, Y \leq c)$ es igual a la probabilidad del evento que contiene todas las salidas dentro del \'area de sombreado fuerte. Otros resultado siguen, por ejemplo:

\vspace{0.2cm}

\[
\mathbb{P}(a < X \leq b, Y \leq c) = F_{X,Y}(b,c) - F_{X,Y}(a,c)
\]

\vspace{0.2cm}

y 

\vspace{0.2cm}

\[
\mathbb{P}( X \leq a, c< Y \leq d) = F_{X,Y}(a,d) - F_{X,Y}(a,c)
\]

\vspace{0.5cm}

\begin{figure}[h]
\centering
\includegraphics[scale=.65]{l2.png}
\end{figure}

\vspace{0.2cm}

El uso de la funci\'on de distribuci\'on acumulativa conjunta de dos variables aleatorias para calcular la probabilidad que $(X, Y)$ est\'e en un rect\'angulo en el plano no es excesivamente complicado. Podemos usar el resultado:

\vspace{0.2cm}

\begin{equation}
\mathbb{P}( a < X \leq b, c< Y \leq d) = F_{X,Y}(b,d) - F_{X,Y}(a,d) - F_{X,Y}(b,c) + F_{X,Y}(a,c).
\end{equation}

\vspace{0.2cm}

Lo que requiere que la funci\'on de distribuci\'on acumulativa conjunta se eval\'ue en cada uno de los cuatro puntos del rect\'angulo. Desafortunadamente, cuando la probabilidad requerida se encuentra en una regi\'on no rectangular, el c\'alculo del  CDF conjunto se vuelve mucho m\'as difÃ­cil de usar. Sin embargo, la ecuaci\'on (2) es un caso especial \'util cuya prueba constituye un ejercicio que ilumina.

\vspace{0.2cm}

Procedemos de la siguiente manera: Sea $A$ que  representa un  rect\'angulo con v\'ertices  $(a, c), (b, c), (b, d)$ y $(a, d)$ (es decir, el rect\'angulo dentro del cual buscamos la  probabilidad que  $X, Y$ pueden ser encontradas). Entonces $\mathbb{P}(A) = \mathbb{P}(a < X \leq b, c  <Y \leq d)$. Sea $B$ el rect\'angulo semi-infinito que se encuentra a la izquierda de $A$  y sea $C$ que  representa el rect\'angulo semi-infinito que est\'a debajo de $A$, como se ilustra en la en la siguiente figura.  

\vspace{0.2cm}

Se debe observar que $A, B$ y $C$ son eventos que no tienen puntos en com\'un, es decir, son mutuamente excluyentes,  as\'i:

\vspace{0.2cm}

\[
\mathbb{P}(A \cup B \cup C) = \mathbb{P}(A) + \mathbb{P}(B) + \mathbb{P}(C),
\]

\vspace{0.2cm}

Lo que da como resultado:

\vspace{0.2cm}

\[
\mathbb{P}(A) = \mathbb{P}(A \cup B \cup C) - \mathbb{P}(B) - \mathbb{P}(C)
\]

\vspace{0.2cm}

Adem\'as de la definic\'on de CDF conjunta, se tienen los siguientes resultados:

\vspace{0.2cm}

\begin{align*}
\mathbb{P}(B) &= F_{X,Y}(a, d) -F_{X,Y}(a,c), \\
\mathbb{P}(C) &= F_{X,Y}(b, c) -F_{X,Y}(a,c), \\
\mathbb{P}(A \cup B \cup C) &= F_{X,Y}(b, d) -F_{X,Y}(a,c), \\
\end{align*}

\vspace{0.2cm}

y podemos concluir que :

\begin{align*}
\mathbb{P}(A)& = F_{X,Y}(b, d) -F_{X,Y}(a,c) - [F_{X,Y}(a, d) -F_{X,Y}(a,c) + F_{X,Y}(b, c) -F_{X,Y}(a,c)]\\
 &=  F_{X,Y}(b, d) -F_{X,Y}(a,d) - F_{X,Y}(b, c) + F_{X,Y}(a,c).
\end{align*}

\vspace{0.2cm}

\begin{figure}[h]
\centering
\includegraphics[scale=.65]{l3.png}
\end{figure}

\vspace{0.2cm}


\begin{ejemplo}
\normalfont La funci\'on de distribuci\'on acumulativa conjunta de dos variables aleatorias continuas  es dada por:

\vspace{0.2cm}

\[
F_{X,Y}(x,y) = (1 - e^{-\lambda x})(1 - e^{-\mu y}),\ 0 \leq x < \infty, 0 \leq y < \infty,
\]

\vspace{0.2cm}

y es cero en otros partes del plano, donde  ambos $\lambda$  y $\mu$ son estrictamente positivo. Calculemos la probabilidad $\mathbb{P}(0 \leq x \leq 1, 0 \leq y \leq 1)$. Aadem\'as por (2) tenemos:

\begin{align*}
\mathbb{P}(0 \leq x \leq 1, 0 \leq y \leq 1) &= F_{X, Y}(1,1) -F_{X,Y}(0,1) -F_{X,Y}(1,0) + F_{X,Y}(0,0)\\
&= (1- e^{-\lambda})(1 - e^{-\mu}) - (1- e^{-\lambda})(1 - e^{0}) -  (1- e^{0})(1 - e^{-\mu}) + (1- e^{0})(1 - e^{0})\\
&= (1 - e^{-\lambda})(1 - e^{-\mu}).
\end{align*}
\end{ejemplo}


\vspace{0.2cm}

Ahora volvemos nuestra atenci\'on a las funciones de distribuci\'on acumulativa marginal derivadas de las CDF conjuntas considerando lo que ocurre cuando a una , pero no a ambas, de las  variables aleatorias $X$ e $Y$ se hace tender al infinito. En el l\'imite cuando $y \rightarrow \infty $, el evento $[X \leq  x, Y \leq y]$ tiende al evento $[X \leq  x, Y < \infty] = [X \leq  x]$ (se ha  utilizado que  $Y < \infty$ para indicar  que $Y$ puede tomar cualquier valor). Esto implica que la \'unica restricci\'on es  $[X \leq  x] $, es decir, el evento $[X \leq x, Y < \infty]$ se reduce al evento $[X \leq x]$. Pero ahora, la probabilidad del evento $[X \leq x]$ es simplemente la distribuci\'on acumulativa de $X$, as\'i:

\vspace{0.2cm}

\[
\lim_{y \rightarrow \infty}F_{X,Y}(x, y) = F_X(x).
\]

\vspace{0.2cm}

De manera similar:

\vspace{0.2cm}


\[
\lim_{x \rightarrow \infty}F_{X,Y}(x, y) = F_Y(y).
\]


\vspace{0.2cm}

Estas funciones son llamadas \texttt{distribuciones marginales} y pueden ser calculadas desde la distribuci\'on conjunta.

\vspace{0.2cm}

\begin{ejemplo}
\normalfont Del ejemplo (1.5), podemos calcular las distribuciones marginales como:

\vspace{0.2cm}

\begin{table}[h]
\centering
\begin{tabular}{l|c c c c c}
  \hline
      & $-2 < x$ & $-2 \leq x < -1$ & $-1 \leq x < 0$ & $0 \leq x < 1$ & $1 \leq x$ \\
      \hline
$F_x(x)$  & 0   &1/8    & 1/4 & 1/2 & 1 \\
\hline
\end{tabular}
\end{table}

\vspace{0.2cm}

para $X$, como antes y para $Y$:

\vspace{0.2cm}

\begin{table}[h]
\centering
\begin{tabular}{l|c c c  c}
  \hline
      & $-2 < y$ & $-2 \leq y  <  0$ & $0 \leq y < 2$ & $y \geq 2$ \\
      \hline
$F_y(y)$  & 0   &1/4    & 3/4 & 1 \\
\hline
\end{tabular}
\end{table}

\end{ejemplo}


\vspace{0.2cm}

De manera similar, las CDF marginales de $X$ y $Y$, derivadas desde la CDF conjunta del ejemplo (1.6), son calculadas f\'acilmente y son:

\vspace{0.2cm}

\begin{align*}
F_X(x) &= (1 -e^{-\lambda x})(1 - 0 ) = (1 -e^{-\lambda x}) \\
F_Y(y) &= (1 - 0)(1 - e^{-\mu y} ) = (1 -e^{-\mu y})
\end{align*}

\vspace{0.2cm}

\begin{defi}
\normalfont Dos variables aleatorias $X$ e $Y$ son independientes si:

\vspace{0.2cm}

\[
F_{X,Y}(x, y) = F_X(x)F_Y(y), \ \ -\infty < x < \infty,\ -\infty < y < \infty.
\]

\vspace{0.2cm}

En otras palabras, si $X$ e $Y$ son variables aleatorias independientes, la funci\'on de distribuci\'on acumulativa conjunta  se factoriza en el producto de sus funciones de distribuci\'on acumulativa marginal. Se puede comprobar que las variables aleatorias $X$ e $Y$ de los ejemplos (1.5) y (1.6) son independientes. Si, adem\'as, $X$ e $Y$ tienen la misma funci\'on  de distribuci\'on (por ejemplo, cuando $\lambda = \mu $ en el ejemplo (1.6)), entonces  el t\'ermino \texttt{ independiente e id\'enticamente distribuido} se le aplica a cada uno de ellos . Normalmente se abreviar\'a a \textbf{iid} o \textbf{IID}.
\end{defi}

\vspace{0.2cm}

La generalizaci\'on de la definici\'on de la funci\'on de distribuci\'on acumulativa conjunta a funciones de distribuci\'on conjunta de m\'as de dos variables aleatorias, denominados \texttt{vectores aleatorios}, es sencilla. Dadas $n$ variables aleatorias $X_1, X_2,\dots X_n$, la  funci\'on de distribuci\'on acumulativa conjunta se define para todo $x_i$ real, $-\infty  < x_i< \infty , i = 1, 2,\dots n$ como:

\vspace{0.2cm}

\[
F(x_1, x_2, \dots, x_n) = \mathbb{P}(X_1 \leq x_1, X_2 \leq x_2, \dots, X_n \leq x_n).
\]

\vspace{0.2cm}

En este caso $F$  debe ser una funci\'on  no decreciente de todos los $x_i, F(-\infty, -\infty, \dots, -\infty) = 0$ y $F(\infty, \infty, \dots, \infty) = 1$. La distribuci\'on marginal de $X_i$ es obtenida desde:

\vspace{0.2cm}

\[
\mathbb{P}(X_i \leq x_i) = F_{X_i}(x_i) = F(\infty, \dots,\infty, x_i, \infty \dots \infty).
\]

\vspace{0.2cm}

Las variables aleatorias $X_1, X_2, \dots X_n$, se dice que son independientes, si para todo los reales $x_1, x_2, \dots x_n$,

\vspace{0.2cm}

\[
F(x_1, x_2, \dots, x_n) = \prod_{i =1}^{n}F_{X_i}(x_i)
\]

\vspace{0.2cm}

Si adem\'as, todas las $n$ variables aleatorias, $X_i, i = 1, 2,\dots, n$, tienen la misma funci\'on de distribuci\'on, se dice que son independientes e id\'enticamente distribuidas.

\vspace{0.2cm}

\begin{ejemplo}
\normalfont  Sean $X$ e $Y$ dos variables aleatorias, cada una tomando los valores $\{\dots, -1,0, 1, \dots \}$ con una funci\'on de masa de probabilidad conjunta:


\[
\mathbb{P}(X =i, Y= j) = p(i,j)\ \ \text{para}\ \ i,j =0,\pm 1, \pm 2, \dots
\]

\vspace{0.2cm}

La funci\'on de distribuci\'on acumulativa conjunta  es dada por:

\vspace{0.2cm}

\[
F_{X,Y}(x,y) = \displaystyle\sum_{i\leq x, j\leq y}p(i,j)\ \ \text{para}\ \ (x, y) \in \mathbb{R}^2
\]

\end{ejemplo}

\vspace{0.2cm}

\begin{ejemplo}
\normalfont  Sean $X$ e $Y$ son variables aleatorias con una funci\'on de distribuci\'on conjunta:

\[
F_{X,Y}(x,y) = \begin{cases}
1 -e^{-x} - e^{-y} + e^{x +y} & \text{si}\ \ x, y \geq 0\\
0 & \text{en otros casos}
\end{cases}
\]

\vspace{0.2cm}

La funci\'on de distribuci\'on marginal de $X$ es:

\vspace{0.2cm}

\[
F_{X}(x) = \lim_{y \rightarrow \infty}F_{X,Y}(x,y) = \begin{cases}
1 -e^{-x} & \text{si}\ \ x \geq 0 \\
0 & \text{en otro casos}
\end{cases}.
\]

\vspace{0.2cm}

As\'i $X$ tiene una distribuci\'on exponencial con par\'ametro $1$. Un c\'alculo muestra que $Y$ tiene esta distribuci\'on tambi\'en. As\'i;

\vspace{0.2cm}

\[
F_{X,Y}(x,y) = F_{X}(x)F_{Y}(y)\ \ \text{para}\ \ x,y \in \mathbb{R}.
\]

\vspace{0.2cm}

por tanto $X$ e $Y$ son independientes.
\end{ejemplo}


\vspace{0.5cm}

\subsection{Funci\'on densidad de probabilidad conjunta }

\vspace{0.2cm}

La funci\'on densidad de probabilidad $f_X(x)$ de una variable aleatoria $X$ continua, es definida como la relaci\'on:

\vspace{0.2cm}

\[
F_X(x) = \bigints_{-\infty}^{x}f_X(t)dt\ \ \text{para}\ \ -\infty < x <  \infty.
\]

\vspace{0.2cm}

Para un peque\~no infinitesimal $dx$, $\mathbb{P}(x < X \leq x  x) = f_X(x) dx$, $\bigints_{-\infty}^{\infty}f_X(x)dx = 1  $ y que la probabilidad $\mathbb{P}(a < X \leq b) = \bigints_{a}^{b}f_X(x)dx$. Todas estas ideas se pueden llevar a dos dimensiones o dimensiones m\'as altas. Si $X$ e $Y$ son variables aleatorias continuas , usualmente podemos encontrar la \texttt{funci\'on densidad de probabilidad conjunta} $f_{X,Y}(x,y)$ tal que:

\vspace{0.2cm}


\[
F_{X, Y}(x,y) = \bigints_{-\infty}^{y}\bigints_{-\infty}^{x}f_{X,Y}(u, v)du dv,
\]

\vspace{0.2cm}

Y satisface, las condiciones $f_{X,Y}(x, y) \geq 0$ para todo $(x, y)$ y :

\vspace{0.2cm}

\[
\bigints_{-\infty}^{\infty}\bigints_{-\infty}^{\infty}f_{X,Y}(u, v)du dv = 1.
\]


\vspace{0.2cm}

Si las derivadas parciales de $F_{X,Y}(x,y)$ con respecto a $x$ e $y$ existen, entonces:

\vspace{0.2cm}

\[
f_{X,Y}(x,y) = \frac{\partial^2 F_{X,Y}(x,y)}{\partial x \partial y}.
\]

\vspace{0.2cm}


Esta relaci\'on  implica que:

\vspace{0.2cm}

\[
\mathbb{P}(a <X \leq b, c <Y \leq d) = \bigints_{a}^{b}\bigints_{c}^{d}f_{X,Y}(x, y)dy dx.
\]

\vspace{0.2cm}

Esto significa que, mientras que la funci\'on de densidad de probabilidad de una \'unica variable aleatoria se mide por unidad de longitud, la densidad de probabilidad conjunta de dos variables aleatorias se mide por unidad de \'area. Adem\'as, para infinitesimales $dx$ y $dy$, tenemos:

\vspace{0.2cm}

\[
\mathbb{P}(x < X \leq x +dx, y < Y \leq y + dy) \approx  f_{X,Y}(x,y)dx dy.
\]

\vspace{0.2cm}

Esta propiedad nos lleva a tomar $f_{X, Y }(x, y)$ como el an\'alogo continuo de $\mathbb{P}(X = x, Y = y)$.

\vspace{0.2cm}

Con dos variables aleatorias conjuntas que son continuas, un evento corresponde a un \'area en el plano $(x, y)$. El evento ocurre si la variable aleatoria $X$  asigna un valor $x_1$ al resultado, $Y$ asigna un valor $y_1$ al resultado y el punto $(x_1, y_1)$ se encuentra dentro del \'area que define el evento. La probabilidad de la ocurrencia de un evento $A$ viene dada por:

\vspace{0.2cm}

\[
\mathbb{P}(A) = \bigints\bigints_{\{x,y|(x,y)\in A \}}f_{X,Y}(u, v)du dv = \bigints\bigints_{A}f_{X,Y}(u, v)du dv.
\]


\vspace{0.3cm}


\begin{ejemplo}
\normalfont Sea $X$ e $Y$ dos variables aleatorias con una funci\'on densidad de probabilidad conjunta, que tiene la forma:

\vspace{0.2cm}

\[
f_{X,Y}(x, y) = \alpha(x + y), \ \ 0 \leq x \leq y \leq 1.
\]

\vspace{0.2cm}

Y es igual a cero, en otros partes. Encontremos el valor de la constante $\alpha$ y dibujemos la regi\'on de probabilidad positiva (el dominio de $f_{X,Y}(x, y)$ en el cual $f_{X,Y}(x, y) > 0$). Tambi\'en, dibujemos la regi\'on, donde $X + Y \leq 1$ e integremos sobre esta regi\'on  para encontrar $\mathbb{P}(X + Y \leq 1)$.

\vspace{0.2cm}

La regi\'on del plano, que contiene la probabilidad positiva es mostrada en la siguiente figura:


\begin{figure}[h]
\centering
\includegraphics[scale=.65]{l5.png}
\end{figure}

\vspace{0.2cm}
 
 Para satisfacer, los requerimientos de una funci\'on de densidad de probabilidad $f_{X,Y}(x,y)$, debe ser negativa y $\bigints_{-\infty}^{\infty}\bigints_{-\infty}^{\infty}f_{X,Y}(x, y)sdx dy$ es igual a $1$. En efecto  $f_{X,Y}(x, y) \geq 0$ para todo $x$ e $y $ y adem\'as que:
 
\begin{align*}
1 = \bigints_{-\infty}^{\infty}\bigints_{-\infty}^{\infty}f_{X,Y}(x, y) dx dy &= \bigints_{0}^{1}\bigints_{0}^{y}\alpha(x + y) dx dy = \bigints_{0}^{1}\biggl(\alpha x^2/2 + \alpha x y\Biggl\vert_{0}^{y} \biggr) dy \\
&= \bigints_{0}^{1}\frac{3\alpha y^2}{2}dy = \alpha/2.
\end{align*}


\vspace{0.2cm}

Se sigue que $\alpha = 2$, para esta funci\'on densidad de probabilidad. La regi\'on, para el cu\'al $X + Y \leq 1$ (dentro de la regi\'on de probabilidad positiva) es mostradas en la siguiente figura. Esta es toda la regi\'on que debemos integrar:

\vspace{0.2cm}

\begin{figure}[h]
\centering
\includegraphics[scale=.65]{l6.png}
\end{figure}

\vspace{0.2cm}

Tenemos:

\vspace{0.2cm}

\begin{align*}
\mathbb{P}(X + Y \leq 1) &= \bigints_{x = 0}^{1/2}\bigints_{y = x}^{1 -x}(2x + 2y)dy dx = \bigints_{x = 0}^{1/2}(2xy + y^2)\biggl\vert_{x}^{1 -x}dx \\
&=  \bigints_{x = 0}^{1/2}[2x[1 -x] + (1 -x)^2 -2x^2 -x^2]dx\\
&=  \bigints_{x = 0}^{1/2}(1 -4x^2)dx =  1/3
\end{align*}
\end{ejemplo}

\vspace{0.2cm}

\begin{ejemplo}
\normalfont Sean $X$ e $Y$ variables aleatorias continuas, que tienen una funci\'on de densidad conjunta:

\vspace{0.2cm}

\[
f_{X,Y}(x,y)= \begin{cases}
cx^2y & \mbox{si}\ \  x^2 \leq y \leq 1\\
0 & \mbox{ en otro caso}.
\end{cases}
\]


\vspace{0.2cm}

Notemos primero que $-1 \leq x \leq 1$. Para encontrar  el valor de $c$,  fijemos un valor $x$ y dejemos que $y$ varie en su rango, el cu\'al es $x^2 \leq y \leq 1$.

\vspace{0.2cm}

\begin{figure}[h]
\centering
\includegraphics[scale=.45]{l4.png}
\end{figure}

\vspace{0.2cm}


As\'i:

\begin{align*}
1 &= \int\int f(x,y)dxdy = c\int_{-1}^{1}\int_{x^2}^{1}x^2ydydx \\
&= c\int_{-1}^{1}x^2\Bigl[\int_{x^2}^{1}ydy\Bigr]dx = c \int_{-1}^{1}x^2\frac{1 -x^4}{2}dx = \frac{4c}{21}.
\end{align*}


\vspace{0.2cm}


Luego el tenemos que  $c = 21/4$. Calculemos $\mathbb{P}(X \geq Y)$:

\vspace{0.2cm}

Esto corresponde al conjunto $A = \{ (x,y); 0 \leq x \leq 1, x^2 \leq y \leq x\}$. (Se puede ver esto, en el gr\'afico anterior ). As\'i:


\vspace{0.2cm}

\begin{align*}
\mathbb{P}(X \geq Y) &= \frac{21}{4}\int_{0}^{1}\int_{x^2}^{x}x^2ydydx = \frac{21}{4}\int_{0}^{1}x^2\Bigl[\int_{x^2}^{x}ydy\Bigr]dx\\\ &= \frac{21}{4}\int_{0}^{1}x^2\Bigl( \frac{x^2 - x^4}{2}\Bigr)dx = \frac{3}{20}.
\end{align*}

\end{ejemplo}

\subsection{Variables aleatorias conjuntas uniformemente distribuidas}

\vspace{0.2cm}

Cuando las variables aleatorias X e Y est\'an uniformemente distribuidas sobre una regi\'on del plano, es posible utilizar argumentos geom\'etricos para calcular ciertas probabilidades. Por ejemplo, si $X$ e $Y$ son variables aleatorias continuas, uniformemente distribuidas sobre el cuadrado unitario y buscamos la probabilidad $X \leq Y$, observamos que la regi\'on $0 \leq  x\leq  1, 0 \leq  y \leq 1$ y $ x \leq y$ constituyen, la mitad de la unidad cuadrada, es decir, el tri\'angulo limitado por el eje $y$, las l\'neas $y = 1$ y $x = y$  y as\'i $\mathbb{P}(X \leq  Y) = 0.5$. De manera general, si $R$ es una regi\'on finita del plano en el que $(X, Y)$ es seguro de pertenecer, la probabilidad de que $(X, Y)$ se encuentre en una subregi\'on $A$ de $R$ es proporcional al tama\~no de $A$, con respecto al  tama\~no de $R$. En otras palabras:

\vspace{0.2cm}

\[
\mathbb{P}\Bigl((X, Y) \in A \Bigr) = \frac{\text{Area de} A}{\text{Area de} R}.
\]


\vspace{0.2cm}

\begin{ejemplo}
\normalfont Sean $X$ e $Y$ variables continuas continuas distribuidas uniformemente  sobre el cuadrado unitario. Entonces:

\vspace{0.2cm}

\[
f_{X,Y}(x,y) = \begin{cases}
1 & \text{si}\ \ 0 \leq x \leq 1, 0 \leq y \leq 1 \\
0 & \text{en otros casos}
\end{cases}
\]

\vspace{0.2cm}

Encontremos $\mathbb{P}(X < 1/2, Y < 1/2)$. El evento $A = \{ X < 1/2, Y < 1/2 \}$ corresponde a un subconjunto del cuadrado unidad. Integrando $f_{X,Y}$ sobre este conjunto, corresponde a calcular el \'area de conjunto $A$ que es $1/4$. As\'i $\mathbb{P}(X < 1/2, Y < 1/2) = 1/4$.
\end{ejemplo}


\vspace{0.2cm}

\begin{ejemplo}
\normalfont Sean $X$ e $Y$ variables aleatorias, uniformemente distribuidas sobre el cuadrado unitario. Encontremos la probabilidad que $\mathbb{P}(X^2 + Y^2 \leq r^2)$ para $r \leq 1$. La funci\'on de distribuci\'ion conjunta de $X$ e $Y$ es dada por:

\vspace{0.2cm}

\[
f_{X,Y}(x, y) = \begin{cases}
1 & 0 \leq x \leq 1, \ 0 \leq y \leq 1, \\
0 & \text{ en otros casos}.
\end{cases}
\]


\vspace{0.2cm}

El \'area de la regi\'on de probabilidad positiva, es decir, el \'area de $R$, es igual a $1$. Se debe notar que $x^2 + y^2 = r^2$ define un c\'irculo centrado en el origen y con  radio $r$. El \'area de este c\'irculo es $\pi r^2$. La porci\'on de la misma que se encuentra en el cuadrante positivo, donde la funci\'on de densidad conjunta es positiva, es $\pi r^2/4$. As\'i, en nuestra terminolog\'ia, el \'area de $A$ es $\pi r^2/4$ y por lo tanto:

\vspace{0.2cm}

\[
\mathbb{P}(X^2 + Y^2 \leq r^2) = \frac{\pi r^2/4}{1} = \frac{\pi r^2}{4}.
\]
\end{ejemplo}

\vspace{0.2cm}

\begin{ejemplo}

\normalfont La funci\'on $f_{X,Y}$  definida por:

\vspace{0.2cm}

\[
f_{X,Y}(x,y) = \begin{cases}
\frac{1}{ab} & \text{si}\ \ 0 < x < b, 0 < y < b \\
0 & \text{en otros casos}
\end{cases}
\]

\vspace{0.2cm}

es una funci\'on densidad de probabilidad conjunta. Las variables $X$ e $Y$ son uniformemente distribuidas sobre el rect\'angulo $B= (0,a) \times (0,b)$. En efecto, para alguna regi\'on del plano $A$:

\vspace{0.2cm}

\begin{align*}
\mathbb{P}((X,Y) \in A) &= \bigintsss\bigintsss_{A}f(x,y)dxdy \\
&= \bigintsss\bigintsss_{A \cap B}f(x,y)dxdy = \dfrac{\text{area}(A \cap B)}{\text{area}(B)}
\end{align*}
\end{ejemplo}

\vspace{0.2cm}

\subsection{Opcional: Suma de variables aleatorias continuas}

\vspace{0.2cm}



\vspace{0.2cm}

Si $X$ e $Y$ son variables aleatorias discretas, con una funci\'on de masa de probabilidad conjunta.?` Cu\'al es la funci\'on de  masa probabilidad de $Z = X + Y$?. Para este caso $Z$ toma valores de $z$ si y s\'olo si $X =x$ y $Y = z -x$ para alg\'un $x$, entonces

\vspace{0.3cm}


\begin{align*}
\mathbb{P}(Z =z) &= \mathbb{P}\Bigl(\bigcup_{x}(\{X =x \} \cap \{Y = z -x \}) \Bigr)\\
 &= \displaystyle\sum_{x \in \texttt{Im X}}\mathbb{P}(X = x, Y = z -x) \ \ \text{para}\ \ z \in \mathbb{R}
\end{align*}

\vspace{0.3cm}

\begin{pros}
\normalfont \texttt{(F\'ormula de Convoluci\'on)} Si $X$ e $Y$ son variables aleatorias discretas independientes, entonces $Z = X +Y$ tiene un funci\'on masa de probabilidad:

\vspace{0.3cm}

\[
\mathbb{P}(Z =z) = \displaystyle\sum_{x \in \texttt{Im X}}\mathbb{P}(X =x)\mathbb{P}(Y = z -x)\ \ \text{para}\ \ z \in \mathbb{R}.
\]

\end{pros}
\vspace{0.3cm}


La f\'ormula de convoluci\'on dice que la funci\'on de masa de $X + Y$ es la \texttt{convoluci\'on} de la funci\'on de $X$ e $Y$.


\vspace{0.3cm}

\begin{defi}
\normalfont La funci\'on indicador de un evento $A$, denotado por $1_A$ y es dado por:

\vspace{0.3cm}

\[
1_{A}(\omega) = \begin{cases}

1 & \text{si}\ \ \omega \in A \\
0 & \text{si}\ \  \omega \notin A
\end{cases}
\]
\end{defi}

\vspace{0.3cm}

La funci\'on $1_A$ indica si o no ocurre $A$. Para esta variable aleatoria discreta, la esperanza es dada por:

\vspace{0.2cm}

\[
\mathbb{E}(1_A) = \mathbb{P}(A)
\]


\vspace{0.3cm}

Algunas propiedades de la funci\'on indicador:

\vspace{0.2cm}

\begin{itemize}
\item $1_{A \cap B} = 1_A1_B$
\item $1_A + 1_{A^c} = 1$
\end{itemize}

\vspace{0.3cm}

\begin{ejemplo}
\normalfont Sean $A_1, A_2, \dots, A_n$ eventos y $1_{A_i}$ la funci\'on indicador de $A_i$. Por propiedad, la uni\'on $A = A_1 \cup A_2 \dots, A_n$ tiene como funci\'on indicador:

\vspace{0.2cm}

\[
1_A = 1 - \prod_{i =1}^{n}(1 -1_{A_i})
\]

\vspace{0.2cm}

El producto puede ser expandido y agrupado para obtener:


\begin{align*}
1_A = \displaystyle\sum_{i}1_{A_i} -  \displaystyle\sum_{i < j}1_{A_i}1_{A_j} +  \displaystyle\sum_{i < j < k}1_{A_i}1_{A_j}1_{A_k} - \cdots \\
+ (-1)^{n +1}1_{A_1}1_{A_2}\cdots 1_{A_n}
\end{align*}

\vspace{0.2cm}

Tomando esperanzas, obtenemos la \texttt{f\'ormula de inclusi\'on-exclusi\'on}:

\vspace{0.2cm}

\begin{align*}
\mathbb{P}\Bigl(\bigcup_{i}A_i \Bigr) = \displaystyle\sum_{i}\mathbb{P}(A_i) - \displaystyle\sum_{i < j}\mathbb{P}(A_i \cap A_j) + \displaystyle\sum_{i< j <k}\mathbb{P}(A_i\cap A_j\cap A_k) - \cdots \\
\cdots + (-1)^{n +1}\mathbb{P}\Bigl(\bigcap_{i}A_i\Bigr)
\end{align*}
\end{ejemplo}

Supongamos que $X$ e $Y$ tienen una densidad $f_{X,Y}$, entonces la densidad de $Z = X +Y$ se puede calcular como:

\vspace{0.2cm}

\begin{align*}
\mathbb{P}(Z \leq z) = \mathbb{P}(X + Y \leq z)\\
= \bigintsss\bigintsss_{A}f_{X,Y}(x,y)dxdy
\end{align*}

\vspace{0.2cm}

 Donde $A = \{(x,y)\in \mathbb{R}^2: x +y \leq z \}$. Expresando esta regi\'on  en los l\'imites de integraci\'on encontramos: 

\vspace{0.2cm}


\begin{align*}
\mathbb{P}(Z \leq z) &=\bigintsss_{x = -\infty}^{\infty}\bigintsss_{y = -\infty}^{z -x}f_{X,Y}(x,y)dxdy\\
&= \bigintsss_{v = -\infty}^{z}\bigintsss_{u = -\infty}^{\infty}f_{X,Y}(u,v -u)dudv
\end{align*}

\vspace{0.2cm}

con la sustituci\'on $u = x, v = x^2 + y$. Diferenciando esta ecuaci\'on con respecto a $z$ , se tiene:

\vspace{0.2cm}

\[
f_{Z}(z) = \bigintsss_{-\infty}^{\infty}f_{X,Y}(u, z -u)du.
\]


\vspace{0.2cm}

Un caso importante acerca de la suma de variables aleatorias es  cuando $X$ e $Y$ son independientes, como indica el siguiente teorema:

\vspace{0.2cm}

\begin{teo}
\normalfont Si las variables aleatorias $X$ e $Y $ son independientes y  continuas con funciones densidad $f_X$ y $f_Y$, entonces la funci\'on densidad de $f_{X,Y}$ de $Z = X + Y$ es

\[
f_{Z}(z) = \bigintsss_{-\infty}^{\infty}f_{X}(x)f_{Y}(z - x)dx \ \ \text{para}\ \ z \in \mathbb{R}.
\]

\end{teo}

\vspace{0.2cm}

\begin{ejemplo}
\normalfont Si $X$ e $Y$ son variables aleatorias independientes, con la distribuci\'on gamma con param\'etros $s$ y $\lambda$ y la distribuci\'on gamma param\'etros $t$ y $\lambda$. Entonces $Z = X +Y$ tiene una funci\'on densidad

\vspace{0.2cm}

\begin{align*}
f_{Z}(z) &= \bigintsss_{\infty}^{\infty}f_{X}(x)f_{Y}(z -x)\\
&= \begin{dcases}
\bigintsss_{0}^{z}f_{X}(x)f_{Y}(z -x)&\ \ \text{si}\ \ z > 0 \\
0 & \text{ en otros casos}
\end{dcases}
\end{align*}


\vspace{0.2cm}


desde que $f_{X}(x)f_{Y}(z -x) = 0$, sin incluir el caso $x > 0$ y $z -x > 0$. Para $z > 0$

\vspace{0.3cm}

\begin{align*}
f_{Z}(z) &= \bigintsss_{0}^{z}\dfrac{1}{\Gamma(z)}\lambda(\lambda x)^{s -1}e^{-\lambda x}\dfrac{1}{\Gamma(t)}\lambda[\lambda(z -x)]^{t -1}e^{-\lambda(z -x)}dx \\
&=Ae^{-\lambda z}\bigintsss_{0}^{z}x^{s -1}(z -x)^{t -1}dx
\end{align*}

\vspace{0.3cm}

donde :


\[
A = \dfrac{1}{\Gamma(s)\Gamma(t)}\lambda^{s + t}
\]

\vspace{0.3cm}

Sustituyendo $y = z/x$ en la \'ultima integral obtenemos:

\vspace{0.3cm}

\[
f_{Z}(z) =Bz^{s +t -1}e^{-\lambda z} \ \ \text{si} \ \ z > 0
\]

\vspace{0.2cm}

donde $B$ es una constante dado por :

\begin{align*}
B = \dfrac{1}{\Gamma(s)\Gamma(t)}\lambda^{s +1}\bigintsss_{0}^{1}y^{s -1}(1 -y)^{t -1}dy
\end{align*}

\vspace{0.2cm}


La \'unica distribuci\'on con funci\'on densidad en este c\'alculo es la distribuci\'on gamma con par\'ametros $s +t$ y $\lambda$ y se sigue que la constante $B$ satisface

\vspace{0.2cm}

\[
B = \dfrac{1}{\Gamma(s +t)}\lambda^{s +t}
\]


\vspace{0.2cm}

Luego, la suma de dos variables aleatorias independientes con un distribuci\'on gamma  con par\'ametros $s, \lambda$ y $t, \lambda$ respectivamente, tiene una distribuci\'on gamma con par\'ametros $s + t, \lambda$.

\vspace{0.2cm}

Comparando las dos \'ultimas ecuaciones tenemos:

\vspace{0.2cm}

\[
\bigintsss_{0}^{1}y^{s -1}(1 -y)^{t -1}dy = \dfrac{\Gamma(s)\Gamma(t)}{\Gamma(s + t)}\ \ \text{para}\ \ s, t> 0
\]
\end{ejemplo}
\end{document}