\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{bigints}
\usepackage[latin1]{inputenc}
\usepackage{geometry}
\usepackage{amsmath} 
\usepackage{mathtools}
%\usepackage[spanish]{babel}
\usepackage{latexsym}
\geometry{verbose,tmargin=1.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage{bm}
\usepackage{graphicx}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bb}[1]{\textbf{#1}}
\DeclareMathOperator{\rank}{\textbf{rango}}
\DeclareMathOperator{\proy}{\textbf{proy}}
\DeclareMathOperator{\nulll}{\textbf{nul}}
\DeclareMathOperator{\diag}{\textbf{diag}}
\DeclareMathOperator{\col}{\textbf{col}}
\DeclareMathOperator{\fila}{\textbf{fila}}
\DeclareMathOperator{\dimm}{dim}
\DeclareMathOperator{\Traz}{Tr}
%\theoremstyle{definition}
\everymath{\displaystyle}
\newtheorem{ejemplo}{{Ejemplo }}[section]
\newtheorem{teo}{{Teorema}}[section]
\newtheorem{defi}{{Definici\'on}}[section]
\newtheorem{pros}{{Proposici\'on}}[section]
\newtheorem{cor}{{Corolario}}[section]
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(ggplot2)
library(grid)
#library(animation)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@

%\title{\underline{\textbf{Notas de mat\'ematica}}}
%\date{}
%\maketitle
\hspace*{0.5\linewidth}
\begin{minipage}{0.6\linewidth}
Curso: C\'alculo de probabilidades CM-1H2\par
Distribuci\'on Normal bivariada \par
\end{minipage}


\vspace {0.5cm}


\section{Normal bivariada general}

\vspace{0.5cm}

Sea $Z_1, Z_2 \sim N(0,1)$ que usaremos para construir una distribuci\'on bivariada normal.

\[
f(z_1, z_2) = \frac{1}{2\pi}\exp\biggl[-\frac{1}{2}(z_1^2 + z_2^2)\biggr],
\]


Queremos transformar estas distribuciones normales para tener los siguientes par\'ametros arbitrarios: $\mu_X, \mu_Y, \sigma_X, \sigma_Y, \rho$.


\begin{align*}
X &= \sigma_XZ_1 + \mu_X \\
Y &= \sigma_Y[\rho Z_1 + \sqrt{1 - \rho^2}Z_2] + \mu_Y
\end{align*}

Primero examinemos las distribuciones marginales  de $X$  e $Y$.

\begin{align*}
X &= \sigma_XZ_1 + \mu_X \\
 &= \sigma_XN(0,1) + \mu_X \\
 &= N(\mu_X, \sigma^2_X)\\
Y &= \sigma_Y[\rho Z_1 + \sqrt{1 - \rho^2}Z_2] + \mu_Y \\
&= \sigma_Y[\rho N(0,1) + \sqrt{1 - \rho^2}N(0,1)] + \mu_Y \\
&= \sigma_Y[N(0,\rho^2) + N(0,1 - \rho^2)] + \mu_Y \\
&= \sigma_YN(0,1) + \mu_Y \\
&= N(\mu_Y,\sigma^2_Y).
\end{align*}

Segundo podemos encontrar la covarianza y el coeficiente de correlaci\'on.

\begin{align*}
\text{Cov}(X,Y) &= \mathbb{E}[(X -\mathbb{E})(Y -\mathbb{E}(Y))] \\
&= \mathbb{E}[(\sigma_X Z_1 + \mu_X - \mu_X)(\sigma_Y[\rho Z_1 + \sqrt{1 -\rho^2}Z_2]+  \mu_Y -\mu_Y)]\\
&= \mathbb{E}[(\sigma_X Z_1)(\sigma_Y[\rho Z_1 + \sqrt{1 -\rho^2}Z_2])] \\
&= \sigma_X \sigma_Y \mathbb{E}[\rho Z^2_1 + \sqrt{1 -\rho^2}Z_1Z_2] \\
&=  \sigma_X \sigma_Y \rho \mathbb{E}(Z^2_1)\\
&=  \sigma_X \sigma_Y \rho
\end{align*}

Por tanto :

\[
\rho(X,Y) = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y} = \rho.
\]

En consecuencia, si queremos generar una variable aleatoria Normal Bivariada con $X \sim N(\mu_X, \sigma^2_X)$ e $Y \sim N(\mu_Y, \sigma^2_Y)$ donde la correlaci\'on de $X$ e $Y$ es $\rho$ podemos generar dos variables aleatorias normales independientes $Z_1$ y $Z_2$ y usar la transformaci\'on:

\begin{align*}
X &= \sigma_XZ_1 + \mu_X \\
Y &= \sigma_Y[\rho Z_1 + \sqrt{1 - \rho^2}Z_2] + \mu_Y
\end{align*}

Tambi\'en podemos usar este resultado para encontrar la densidad conjunta de la Bivariada Normal utilizando un cambio de variables en dos dimensiones.

\section{Cambio de variable multivariado}

Sea $X_1,\dots, X_n$ tiene una distribuci\'on conjunta continua con PDF $f$ definido en S. Podemos definir $n$ nuevas variables aleatorias $Y_1,\dots, Y_n$ como sigue:

\[
Y_1 = r_1(X_1, \dots, X_n) \quad Y_n = r_n(X_1, \dots, X_n) 
\]

Si asumimos que las $n$ funciones $r_1,\dots r_n$ definen una transformaci\'on diferenciable  uno a uno de $S$ a $T$, entonces sea  inversa de esta transformaci\'on:

\[
x_1 = s_1(y_1, \dots, y_n) \quad x_n = s_n(y_1,\dots, y_n)
\]

Entonces e PDF conjunto  de $Y_1,\dots Y_n$ es,

\[
g(y_1,\dots, y_n) = \begin{cases}
f(s_1,\dots ,s_n)\vert J \vert & \text{para}\ (y_1,\dots, y_n) \in T \\
0 & \text{en otros casos}
\end{cases}
\]

donde 

\[
J = \det \begin{bmatrix}
\frac{\partial x_1}{\partial y_1} & \cdots & \frac{\partial x_1}{\partial y_n}\\
\vdots & \ddots & \vdots \\
\frac{\partial x_n}{\partial y_1} & \cdots & \frac{\partial x_n}{\partial y_n}
\end{bmatrix}
\]


Lo primero que tenemos que encontrar son las inversas de la transformaci\'on. Si $x = r_1 (z1, z_2)$ y $y = r_2 (z_1, z_2)$ necesitamos encontrar las funciones $h_1$ y $h_2$ de manera que $Z_1 = s_1(X, Y)$ y $Z_2 = s_2(X, Y)$.


\begin{align*}
X &= \sigma_XZ_1 + \mu_X \\
Z_1 &= \frac{X -\mu_X}{\sigma_X}\\
Y &= \sigma_Y[\rho Z_1 + \sqrt{1 - \rho^2}Z_2] + \mu_Y \\
\frac{Y -\mu_Y}{\sigma_Y} &= \rho\frac{X -\mu_X}{\sigma_X} + \sqrt{1 - \rho^2}Z_2] \\
Z_2 &= \frac{1}{\sqrt{1 - \rho^2}}\biggl[\frac{Y - \mu_Y}{\sigma_Y} - \rho\frac{X -\mu_X}{\sigma_X} \biggr].
\end{align*}

Por tanto,

\[
s_1(x, y) = \frac{x -\mu_X}{\sigma_X}\quad s_2(x) = \frac{1}{\sqrt{1 - \rho^2}}\biggl[\frac{y  - \mu_Y}{\sigma_Y} - \rho\frac{x -\mu_X}{\sigma_X} \biggr]
\]

Ahora calculamos el Jacobiano,

\[
J = \det \begin{bmatrix}
\frac{\partial s_1}{\partial x} &  \frac{\partial s_1}{\partial y} \\
\frac{\partial s_2}{\partial x} & \frac{\partial s_2}{\partial y}
\end{bmatrix}
= \det \begin{bmatrix}
\frac{1}{\sigma_X} &  0 \\
\frac{-\rho}{\sigma_X\sqrt{1 -\rho^2}} & \frac{1}{\sigma_Y\sigma_X\sqrt{1 -\rho^2}}
\end{bmatrix} = \frac{1}{\sigma_X \sigma_Y\sqrt{1 -\rho^2}}.
\]

La densidad conjunta de $X$ e $Y $ es dado por,

\begin{align*}
f(x, y) &= f(z_1, z_2)\vert J \vert  \\
 \frac{1}{2\pi}\exp\biggl[-\frac{1}{2}(z_1^2 + z_2^2)\biggr]\vert J \vert &=  \frac{1}{2\pi \sigma_X \sigma_Y\sqrt{1 -\rho^2}}\exp\biggl[-\frac{1}{2}(z_1^2 + z_2^2)\biggr] \\
 &= \frac{1}{2\pi \sigma_X \sigma_Y\sqrt{1 -\rho^2}}\exp\biggl[-\frac{1}{2} \biggl[\biggl(\frac{x - \mu_X}{\sigma_X} \biggr)^2 + \frac{1}{1 -\rho^2}\biggl(\frac{y -\mu_Y}{\sigma_Y} - \rho\frac{x - \mu_X}{\sigma_X} \biggr)^2 \biggr] \biggr]\\
  &=\frac{1}{2\pi \sigma_X \sigma_Y(1 -\rho^2)^{1/2}}\exp\biggl[\frac{-1}{2(1 -\rho^2)}\biggl(\frac{(x - \mu_X)^2}{\sigma^2_X} + \frac{(y -\mu_Y)^2}{\sigma^2_Y} - 2\rho\frac{(x -\mu_X)}{\sigma_X}\frac{(y -\mu_Y)}{\sigma_Y} \biggr) \biggr].
\end{align*}

\section{ Densidad de una distribuci\'on normal bivariada- Notaci\'on Matricial}

Obviamente, la densidad para la Normal Bivariada es complicada y solo empeora cuando consideramos densidades conjuntas  de distribuciones  normales de dimensiones m\'as altas. Podemos escribir la densidad en una forma m\'as compacta usando la notaci\'on matricial,


\[
\textbf{x} = \begin{pmatrix}
x \\
y
\end{pmatrix}\quad \bm{\mu} = \begin{pmatrix}
\mu_X \\
\mu_Y
\end{pmatrix}
 \quad \bm{\Sigma} = \begin{pmatrix}
\sigma^2_X & \rho \sigma_X\sigma_Y \\
 \rho \sigma_X\sigma_Y  & \sigma^2_Y
\end{pmatrix}
\]

\[
f(\bm{x}) = \frac{1}{2\pi}(\det \bm{\Sigma})^{-1/2}\exp \biggl[-\frac{1}{2}(\bm{x} -\bm{\mu})^T\bm{\Sigma}^{-1}(\bm{x} -\bm{\mu}) \biggr]
\]

\vspace{0.2cm}

Podemos confirmar nuestros resultados verificando el valor de $(\det \bm{\Sigma})^{-1/2}$ y $(\bm{x} -\bm{\mu})^T\bm{\Sigma}^{-1}(\bm{x} -\bm{\mu})$ para el caso bivariado.

\vspace{0.2cm}

\[
(\det \bm{\Sigma})^{-1/2} = (\sigma^2_X\sigma^2_Y - \rho^2\sigma^2_X\sigma^2_Y)^{-1/2} = \frac{1}{\sigma_X\sigma_Y(1 -\rho^2)^{1/2}}
\]

\vspace{0.2cm}

Tambi\'en,

\[
(\bm{x} -\bm{\mu})^T\bm{\Sigma}^{-1}(\bm{x} -\bm{\mu})
\]

\begin{align*}
&=  \frac{1}{\sigma^2_X\sigma^2_Y(1 -\rho^2)}\begin{pmatrix}
x -\mu_X \\
y -\mu_Y
\end{pmatrix}^T \begin{pmatrix}
\sigma^2_Y & -\rho\sigma_X\sigma_Y \\
-\rho\sigma_X\sigma_Y & \sigma^2_X
\end{pmatrix}\begin{pmatrix}
x -\mu_X \\
y -\mu_Y
\end{pmatrix} \\
&=\frac{1}{\sigma^2_X\sigma^2_Y(1 -\rho^2)}\begin{pmatrix}
\sigma^2_Y(x -\mu_X) -\rho\sigma_X\sigma_Y(y -\mu_Y) \\
-\rho\sigma_X\sigma_Y(x -\mu_X) + \sigma^2_X(y -\mu_Y)
\end{pmatrix}^T 
\begin{pmatrix}
x -\mu_x \\
y -\mu_y
\end{pmatrix} \\
&= \frac{1}{\sigma^2_X\sigma^2_Y(1 -\rho^2)}(\sigma^2_Y(x -\mu_X)^2  - 2\rho\sigma_X\sigma_Y(x -\mu_X)(y- \mu_Y) +  \sigma^2_X(y -\mu_Y)^2) \\
&= \frac{1}{1 -\rho^2}\biggl(\frac{(x -\mu_X)^2}{\sigma^2_X} - 2\rho\frac{(x -\mu_X)(y -\mu_Y)}{\sigma_X\sigma_Y} + \frac{(y - \mu_Y)^2}{\sigma^2_Y} \biggr).
\end{align*}


\vspace{0.2cm}


\subsection{Ejemplos}

La distribuci\'on normal est\'andar bivariada tiene un m\'aximo en el origen. Debes tener  en cuenta que el \'unico par\'ametro en la distribuci\'on normal est\'andar bivariada es la correlaci\'on $\rho$ entre $x$ e $y$. Si $x$ e $y$ son independientes ($\rho = 0$), las superficies de la constante $f(x, y)$ son c\'irculos conc\'entricos alrededor del origen. A medida que aumenta $\rho$, la distribuci\'on se estira diagonalmente, formando isol\'ineas el\'ipticas con ejes mayores con pendiente positiva. Para $\rho$ negativo, los ejes mayores tienen una pendiente negativa.
\end{document}