\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{bigints}
\usepackage[latin1]{inputenc}
\usepackage{geometry}
\usepackage{amsmath} 
\usepackage{mathtools}
%\usepackage[spanish]{babel}
\usepackage{latexsym}
\geometry{verbose,tmargin=1.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{float}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bb}[1]{\textbf{#1}}
\DeclareMathOperator{\rank}{\textbf{rango}}
\DeclareMathOperator{\proy}{\textbf{proy}}
\DeclareMathOperator{\nulll}{\textbf{nul}}
\DeclareMathOperator{\diag}{\textbf{diag}}
\DeclareMathOperator{\col}{\textbf{col}}
\DeclareMathOperator{\fila}{\textbf{fila}}
\DeclareMathOperator{\dimm}{dim}
\DeclareMathOperator{\Traz}{Tr}
%\theoremstyle{definition}
\everymath{\displaystyle}
\newtheorem{ejemplo}{{Ejemplo }}[section]
\newtheorem{teo}{{Teorema}}[section]
\newtheorem{defi}{{Definici\'on}}[section]
\newtheorem{pros}{{Proposici\'on}}[section]
\newtheorem{cor}{{Corolario}}[section]
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(ggplot2)
library(grid)
#library(animation)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@

%\title{\underline{\textbf{Notas de mat\'ematica}}}
%\date{}
%\maketitle
\hspace*{0.5\linewidth}
\begin{minipage}{0.6\linewidth}
Curso: C\'alculo de probabilidades CM-1H2\par
Distribuci\'on Normal bivariada \par
\end{minipage}


\vspace {0.5cm}


\section{Normal bivariada general}

Sea $Z_1, Z_2 \sim N(0,1)$ que usaremos para construir una distribuci\'on bivariada normal.

\[
f(z_1, z_2) = \frac{1}{2\pi}\exp\biggl[-\frac{1}{2}(z_1^2 + z_2^2)\biggr],
\]


Queremos transformar estas distribuciones normales para tener los siguientes par\'ametros arbitrarios: $\mu_X, \mu_Y, \sigma_X, \sigma_Y, \rho$.


\begin{align*}
X &= \sigma_XZ_1 + \mu_X \\
Y &= \sigma_Y[\rho Z_1 + \sqrt{1 - \rho^2}Z_2] + \mu_Y
\end{align*}

Primero examinemos las distribuciones marginales  de $X$  e $Y$.

\begin{align*}
X &= \sigma_XZ_1 + \mu_X \\
 &= \sigma_XN(0,1) + \mu_X \\
 &= N(\mu_X, \sigma^2_X)\\
Y &= \sigma_Y[\rho Z_1 + \sqrt{1 - \rho^2}Z_2] + \mu_Y \\
&= \sigma_Y[\rho N(0,1) + \sqrt{1 - \rho^2}N(0,1)] + \mu_Y \\
&= \sigma_Y[N(0,\rho^2) + N(0,1 - \rho^2)] + \mu_Y \\
&= \sigma_YN(0,1) + \mu_Y \\
&= N(\mu_Y,\sigma^2_Y).
\end{align*}

Segundo podemos encontrar la covarianza y el coeficiente de correlaci\'on.

\begin{align*}
\text{Cov}(X,Y) &= \mathbb{E}[(X -\mathbb{E})(Y -\mathbb{E}(Y))] \\
&= \mathbb{E}[(\sigma_X Z_1 + \mu_X - \mu_X)(\sigma_Y[\rho Z_1 + \sqrt{1 -\rho^2}Z_2]+  \mu_Y -\mu_Y)]\\
&= \mathbb{E}[(\sigma_X Z_1)(\sigma_Y[\rho Z_1 + \sqrt{1 -\rho^2}Z_2])] \\
&= \sigma_X \sigma_Y \mathbb{E}[\rho Z^2_1 + \sqrt{1 -\rho^2}Z_1Z_2] \\
&=  \sigma_X \sigma_Y \rho \mathbb{E}(Z^2_1)\\
&=  \sigma_X \sigma_Y \rho
\end{align*}

Por tanto :

\[
\rho(X,Y) = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y} = \rho.
\]

En consecuencia, si queremos generar una variable aleatoria Normal Bivariada con $X \sim N(\mu_X, \sigma^2_X)$ e $Y \sim N(\mu_Y, \sigma^2_Y)$ donde la correlaci\'on de $X$ e $Y$ es $\rho$ podemos generar dos variables aleatorias normales independientes $Z_1$ y $Z_2$ y usar la transformaci\'on:

\begin{align*}
X &= \sigma_XZ_1 + \mu_X \\
Y &= \sigma_Y[\rho Z_1 + \sqrt{1 - \rho^2}Z_2] + \mu_Y
\end{align*}

Tambi\'en podemos usar este resultado para encontrar la densidad conjunta de la Bivariada Normal utilizando un cambio de variables en dos dimensiones.

\section{Cambio de variable multivariado}

Sea $X_1,\dots, X_n$ tiene una distribuci\'on conjunta continua con PDF $f$ definido en S. Podemos definir $n$ nuevas variables aleatorias $Y_1,\dots, Y_n$ como sigue:

\[
Y_1 = r_1(X_1, \dots, X_n) \quad Y_n = r_n(X_1, \dots, X_n) 
\]

Si asumimos que las $n$ funciones $r_1,\dots r_n$ definen una transformaci\'on diferenciable  uno a uno de $S$ a $T$, entonces sea  inversa de esta transformaci\'on:

\[
x_1 = s_1(y_1, \dots, y_n) \quad x_n = s_n(y_1,\dots, y_n)
\]

Entonces e PDF conjunto  de $Y_1,\dots Y_n$ es,

\[
g(y_1,\dots, y_n) = \begin{cases}
f(s_1,\dots ,s_n)\vert J \vert & \text{para}\ (y_1,\dots, y_n) \in T \\
0 & \text{en otros casos}
\end{cases}
\]

donde 

\[
J = \det \begin{bmatrix}
\frac{\partial x_1}{\partial y_1} & \cdots & \frac{\partial x_1}{\partial y_n}\\
\vdots & \ddots & \vdots \\
\frac{\partial x_n}{\partial y_1} & \cdots & \frac{\partial x_n}{\partial y_n}
\end{bmatrix}
\]


Lo primero que tenemos que encontrar son las inversas de la transformaci\'on. Si $x = r_1 (z1, z_2)$ y $y = r_2 (z_1, z_2)$ necesitamos encontrar las funciones $h_1$ y $h_2$ de manera que $Z_1 = s_1(X, Y)$ y $Z_2 = s_2(X, Y)$.


\begin{align*}
X &= \sigma_XZ_1 + \mu_X \\
Z_1 &= \frac{X -\mu_X}{\sigma_X}\\
Y &= \sigma_Y[\rho Z_1 + \sqrt{1 - \rho^2}Z_2] + \mu_Y \\
\frac{Y -\mu_Y}{\sigma_Y} &= \rho\frac{X -\mu_X}{\sigma_X} + \sqrt{1 - \rho^2}Z_2] \\
Z_2 &= \frac{1}{\sqrt{1 - \rho^2}}\biggl[\frac{Y - \mu_Y}{\sigma_Y} - \rho\frac{X -\mu_X}{\sigma_X} \biggr].
\end{align*}

Por tanto,

\[
s_1(x, y) = \frac{x -\mu_X}{\sigma_X}\quad s_2(x) = \frac{1}{\sqrt{1 - \rho^2}}\biggl[\frac{y  - \mu_Y}{\sigma_Y} - \rho\frac{x -\mu_X}{\sigma_X} \biggr]
\]

Ahora calculamos el Jacobiano,

\[
J = \det \begin{bmatrix}
\frac{\partial s_1}{\partial x} &  \frac{\partial s_1}{\partial y} \\
\frac{\partial s_2}{\partial x} & \frac{\partial s_2}{\partial y}
\end{bmatrix}
= \det \begin{bmatrix}
\frac{1}{\sigma_X} &  0 \\
\frac{-\rho}{\sigma_X\sqrt{1 -\rho^2}} & \frac{1}{\sigma_Y\sigma_X\sqrt{1 -\rho^2}}
\end{bmatrix} = \frac{1}{\sigma_X \sigma_Y\sqrt{1 -\rho^2}}.
\]

La densidad conjunta de $X$ e $Y $ es dado por,

\begin{align*}
f(x, y) &= f(z_1, z_2)\vert J \vert  \\
 \frac{1}{2\pi}\exp\biggl[-\frac{1}{2}(z_1^2 + z_2^2)\biggr]\vert J \vert &=  \frac{1}{2\pi \sigma_X \sigma_Y\sqrt{1 -\rho^2}}\exp\biggl[-\frac{1}{2}(z_1^2 + z_2^2)\biggr] \\
 &= \frac{1}{2\pi \sigma_X \sigma_Y\sqrt{1 -\rho^2}}\exp\biggl[-\frac{1}{2} \biggl[\biggl(\frac{x - \mu_X}{\sigma_X} \biggr)^2 + \frac{1}{1 -\rho^2}\biggl(\frac{y -\mu_Y}{\sigma_Y} - \rho\frac{x - \mu_X}{\sigma_X} \biggr)^2 \biggr] \biggr]\\
  &=\frac{1}{2\pi \sigma_X \sigma_Y(1 -\rho^2)^{1/2}}\exp\biggl[\frac{-1}{2(1 -\rho^2)}\biggl(\frac{(x - \mu_X)^2}{\sigma^2_X} + \frac{(y -\mu_Y)^2}{\sigma^2_Y} - 2\rho\frac{(x -\mu_X)}{\sigma_X}\frac{(y -\mu_Y)}{\sigma_Y} \biggr) \biggr].
\end{align*}

\section{ Densidad de una distribuci\'on normal bivariada- Notaci\'on Matricial}

Obviamente, la densidad para la Normal Bivariada es complicada y solo empeora cuando consideramos densidades conjuntas  de distribuciones  normales de dimensiones m\'as altas. Podemos escribir la densidad en una forma m\'as compacta usando la notaci\'on matricial,


\[
\textbf{x} = \begin{pmatrix}
x \\
y
\end{pmatrix}\quad \bm{\mu} = \begin{pmatrix}
\mu_X \\
\mu_Y
\end{pmatrix}
 \quad \bm{\Sigma} = \begin{pmatrix}
\sigma^2_X & \rho \sigma_X\sigma_Y \\
 \rho \sigma_X\sigma_Y  & \sigma^2_Y
\end{pmatrix}
\]

\[
f(\bm{x}) = \frac{1}{2\pi}(\det \bm{\Sigma})^{-1/2}\exp \biggl[-\frac{1}{2}(\bm{x} -\bm{\mu})^T\bm{\Sigma}^{-1}(\bm{x} -\bm{\mu}) \biggr]
\]

\vspace{0.2cm}

Podemos confirmar nuestros resultados verificando el valor de $(\det \bm{\Sigma})^{-1/2}$ y $(\bm{x} -\bm{\mu})^T\bm{\Sigma}^{-1}(\bm{x} -\bm{\mu})$ para el caso bivariado.

\vspace{0.2cm}

\[
(\det \bm{\Sigma})^{-1/2} = (\sigma^2_X\sigma^2_Y - \rho^2\sigma^2_X\sigma^2_Y)^{-1/2} = \frac{1}{\sigma_X\sigma_Y(1 -\rho^2)^{1/2}}
\]

\vspace{0.2cm}

Tambi\'en,

\[
(\bm{x} -\bm{\mu})^T\bm{\Sigma}^{-1}(\bm{x} -\bm{\mu})
\]

\begin{align*}
&=  \frac{1}{\sigma^2_X\sigma^2_Y(1 -\rho^2)}\begin{pmatrix}
x -\mu_X \\
y -\mu_Y
\end{pmatrix}^T \begin{pmatrix}
\sigma^2_Y & -\rho\sigma_X\sigma_Y \\
-\rho\sigma_X\sigma_Y & \sigma^2_X
\end{pmatrix}\begin{pmatrix}
x -\mu_X \\
y -\mu_Y
\end{pmatrix} \\
&=\frac{1}{\sigma^2_X\sigma^2_Y(1 -\rho^2)}\begin{pmatrix}
\sigma^2_Y(x -\mu_X) -\rho\sigma_X\sigma_Y(y -\mu_Y) \\
-\rho\sigma_X\sigma_Y(x -\mu_X) + \sigma^2_X(y -\mu_Y)
\end{pmatrix}^T 
\begin{pmatrix}
x -\mu_x \\
y -\mu_y
\end{pmatrix} \\
&= \frac{1}{\sigma^2_X\sigma^2_Y(1 -\rho^2)}(\sigma^2_Y(x -\mu_X)^2  - 2\rho\sigma_X\sigma_Y(x -\mu_X)(y- \mu_Y) +  \sigma^2_X(y -\mu_Y)^2) \\
&= \frac{1}{1 -\rho^2}\biggl(\frac{(x -\mu_X)^2}{\sigma^2_X} - 2\rho\frac{(x -\mu_X)(y -\mu_Y)}{\sigma_X\sigma_Y} + \frac{(y - \mu_Y)^2}{\sigma^2_Y} \biggr).
\end{align*}


\vspace{0.2cm}


La distribuci\'on normal est\'andar bivariada tiene un m\'aximo en el origen. Debes tener  en cuenta que el \'unico par\'ametro en la distribuci\'on normal est\'andar bivariada es la correlaci\'on $\rho$ entre $x$ e $y$. Si $x$ e $y$ son independientes ($\rho = 0$), las superficies de la constante $f(x, y)$ son c\'irculos conc\'entricos alrededor del origen. A medida que aumenta $\rho$, la distribuci\'on se estira diagonalmente, formando isol\'ineas el\'ipticas con ejes mayores con pendiente positiva. Para $\rho$ negativo, los ejes mayores tienen una pendiente negativa.


\vspace{0.5cm}

Estos gr\'aficos fueron tomados desde las notas de Colin Rundel de la universidad de Duke.
\vspace{0.2cm}

\begin{figure}[H]
\centering
\includegraphics[scale=.55]{c1.png}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=.55]{c2.png}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=.55]{c3.png}
\end{figure}


\begin{figure}[H]
\centering
\includegraphics[scale=.55]{c4.png}
\end{figure}


\subsection{Distribuci\'on de probabilidad condicional normal bivariada}

Una caracter\'istica poderosa de la distribuci\'on normal bivariada es que la funci\'on de distribuci\'on de probabilidad condicional para una de las variables, dado un valor conocido para la otra variable, es distribuida normalmente, es decir, $f(x|y = y_0) \sim N(\mu_{X|y = y_0}, \sigma^2_{X|y = y_0})$. La media condicional y la varianza de $x$, dado  $y = y_0$ es:

\begin{align*}
\mu_{X|y = y_0} &= \mu_X + \rho \sigma_X\frac{(y - y_{0})}{\sigma_y} \\
 \sigma^2_{X|y = y_0} &= \sigma_X\sqrt{1 - \rho^2}.
\end{align*}

\vspace{0.2cm}

A partir de estos par\'ametros, podemos determinar la probabilidad de que $x$ caiga en un rango dado,  $x_0 \leq x \leq x_1$ de la distribuci\'on normal.

Este punto se ilustra en la siguiente figura, donde la distribuci\'on condicional para la variable $x$, dada  la variable $y = 1.5$. El sombreado indica la probabilidad de que $x$ exceda $1.5$ la desviaci\'on est\'ndar si no se sabe nada sobre $y$ (sombreado oscuro para la curva inferior), y si se sabe que $y = 1.54$ (sombreado claro para la curva superior). 

\vspace {0.2cm}

Ten en cuenta la gran diferencia en las probabilidades, aunque la correlaci\'on no es excepcionalmente alta.


\begin{figure}[H]
\centering
\includegraphics[scale=.35]{c5.png}
\end{figure}

\vspace{0.3cm}

Hay un par de caracter\'isticas adicionales a tener en cuenta. Primero, el eje mayor de la elipse definido por la distribuci\'on normal bivariada (ver la siguiente figura donde $\rho = 0.4$) puede obtenerse mediante regresi\'on lineal de m\'inimos cuadrados. En este caso, se toma la regresi\'on de m\'inimos cuadrados para minimizar la distancia perpendicular entre los datos y la l\'inea de regresi\'on. Esto se puede lograr con el an\'alisis de la funci\'on ortogonal emp\'irica (empirical orthogonal function (EOF) ), en ese an\'alisis de regresi\'on simplemente gira el sistema de coordenadas.

\vspace{0.2cm}

El vector propio principal del an\'alisis EOF apunta en la direcci\'on del eje mayor de la elipse y la distancia desde el m\'aximo de la distribuci\'on a una isol\'inea de la distribuci\'on.



\begin{figure}[H]
\centering
\includegraphics[scale=.35]{c6.png}
\end{figure}


\section{Distribuci\'on normal multivariada}

La notaci\'on matricial nos permite expresar f\'acilmente la densidad de la distribuci\'on normal multivariada para un n\'umero arbitrario de dimensiones. Expresamos la distribuci\'on normal multivariable $k$ dimensional como sigue,

\[
\bm{X} \sim N_k(\bm{\mu},\bm{\Sigma})
\]

donde $\bm{\mu}$ es el vector columna $k \times 1$ de medias y $\bm{\Sigma}$ es la matriz de covarianza $k \times k$ donde $\{\bm{\Sigma}\}_{i,j} = \text{Cov}(X_i, X_j)$.


\vspace{0.2cm}

La densidad de la distribuci\'on es

\[
f(\bm{x}) = \frac{1}{(2\pi)^{k/2}}(\det \bm{\Sigma})^{-1/2}\exp\biggl[-\frac{1}{2}(\bm{x}- \bm{\mu})^T\Sigma^{-1}(\bm{x}- \bm{\mu}) \biggr],
\]

\vspace{0.2cm}

En el caso bivariado, tuvimos una transformaci\'on de modo que pudimos generar dos valores aleatorias normales  independientes y transformarlos en una muestra de una distribuci\'on normal bivariada arbitraria.

\vspace{0.2cm}

Existe un m\'etodo similar para la distribuci\'on normal multivariable que aprovecha la descomposici\'on de Cholesky de la matriz de covarianza. La descomposici\'on de Cholesky se define para una matriz $X$ definida positiva y sim\'etrica como,

\[
L = \text{Cholesky}(\bm{X})
\]

\vspace{0.2cm}

donde $\bm{L}$ es una matriz triangular inferior tal que $\bm{L}\bm{L}^T = \bm{X}$.
\end{document}